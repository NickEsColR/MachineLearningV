{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NickEsColR/MachineLearningV/blob/train/taller/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equipo**\n",
        "\n",
        "- Nicolás Colmenares\n",
        "\n",
        "- Carlos Martinez"
      ],
      "metadata": {
        "id": "Ckrr43Eb7-jD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Situación:**\n",
        "Una ciudad enfrenta un aumento significativo de casos de dengue, con una tasa de incidencia que supera el promedio nacional.\n",
        "La anticipación de brotes es crucial para implementar medidas preventivas y reducir la propagación de la enfermedad.\n",
        "\n",
        "**Objetivo:**\n",
        "Desarrollar un modelo predictivo utilizando redes neuronales para pronosticar futuros brotes de dengue en cada barrio de la ciudad.\n",
        "Utilizar una base de datos histórica de casos de dengue desde 2015 hasta 2022 para entrenar el modelo.\n",
        "Anticiparse a los brotes con al menos 3 semanas de anticipación.\n",
        "\n",
        "**Finalidad:**\n",
        "Permitir a las autoridades de salud pública tomar acciones oportunas, como:\n",
        "Preparar a las instituciones prestadoras de salud (IPS).\n",
        "Gestionar recursos (carros fumigadores, limpieza de sumideros).\n",
        "Capacitar a la comunidad."
      ],
      "metadata": {
        "id": "zQ9T88GEx2Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Redes Neuronales Tradicinales (MLP)\n",
        "2. Red Convolucional (CNN) adaptada a series temporales\n",
        "3. Red Neuronal Recurrente (RNN) básica.\n",
        "4. Modelo con LSTMs\n",
        "5. Modelo con GRUs"
      ],
      "metadata": {
        "id": "dVtVgKGCyXfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diccionario"
      ],
      "metadata": {
        "id": "R3PIeauF9c56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.parquet - El conjunto de datos de entrenamiento\n",
        "test.parquet - El conjunto de datos de prueba\n",
        "sample_submission.csv - un ejemplo de un archivo a someter en la competencia"
      ],
      "metadata": {
        "id": "uwGcqMDn-AoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Variable**         | **Descripción**                                                                                      |\n",
        "|-----------------------|------------------------------------------------------------------------------------------------------|\n",
        "| id_bar               | identificador único del barrio                                                                      |\n",
        "| anio                 | Año de ocurrencia                                                                                   |\n",
        "| semana               | Semana de ocurrencia                                                                               |\n",
        "| Estrato              | Estrato socioeconómico del barrio                                                                   |\n",
        "| area_barrio          | Área del barrio en km²                                                                             |\n",
        "| dengue               | Conteo de casos de dengue                                                                          |\n",
        "| concentraciones      | Cantidad de visitas e intervención a lugares de concentración humana (Instituciones)                |\n",
        "| vivienda             | Conteo de las visitas a viviendas a revisión y control de criaderos                                 |\n",
        "| equipesado           | Conteo de las fumigaciones con Maquinaria Pesada                                                   |\n",
        "| sumideros            | Conteo de las intervenciones a los sumideros                                                       |\n",
        "| maquina              | Conteo de las fumigaciones con motomochila                                                         |\n",
        "| lluvia_mean          | Lluvia promedio en la semana i                                                                     |\n",
        "| lluvia_var           | Varianza de la lluvia en la semana i                                                               |\n",
        "| lluvia_max           | Lluvia máxima en la semana i                                                                       |\n",
        "| lluvia_min           | Lluvia mínima en la semana i                                                                       |\n",
        "| temperatura_mean     | Temperatura promedio en la semana i                                                                |\n",
        "| temperatura_var      | Varianza de la temperatura en la semana i                                                          |\n",
        "| temperatura_max      | Temperatura máxima en la semana i                                                                  |\n",
        "| temperatura_min      | Temperatura mínima en la semana i                                                                  |\n"
      ],
      "metadata": {
        "id": "wKLm80Huf2vY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0.  Configuraciones de Colab"
      ],
      "metadata": {
        "id": "l9U_uqnvvYtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mover Kaggle.json a la ubicación correcta después de subirlo"
      ],
      "metadata": {
        "id": "zi1MbjLG81SQ"
      }
    },
    {
      "source": [
        "#Estas líneas son comandos de shell que se ejecutan dentro del Jupyter notebook. Se usan para configurar las credenciales de la API de Kaggle, que son necesarias para descargar conjuntos de datos (datasets) desde Kaggle.\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FkW9-t478uLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/kaggle/output\n",
        "!rm -rf /content/kaggle/input"
      ],
      "metadata": {
        "id": "-YMKmQruoaHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargar dataset de la competencia"
      ],
      "metadata": {
        "id": "VWH9Y2UG9KLD"
      }
    },
    {
      "source": [
        "!kaggle competitions download -c aa-v-2025-i-pronosticos-nn-rnn-cnn"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SnUZl0d-8_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/kaggle/output\n",
        "!mkdir -p /content/kaggle/input"
      ],
      "metadata": {
        "id": "vaLprrm32zGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv aa-v-2025-i-pronosticos-nn-rnn-cnn.zip /content/kaggle/input"
      ],
      "metadata": {
        "id": "s07nq5K1zyuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/kaggle/input/aa-v-2025-i-pronosticos-nn-rnn-cnn.zip -d /content/kaggle/input/"
      ],
      "metadata": {
        "id": "skxqP_oq0VeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/kaggle/input\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "0m2oYgGOxJue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1.   Imports\n",
        "\n"
      ],
      "metadata": {
        "id": "FiQLr9h-0Cr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta # Importing the required modules datetime and timedelta\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
        "import copy\n",
        "from typing import List, Tuple, Type, Any, Dict, Union\n",
        "import sys"
      ],
      "metadata": {
        "id": "MO0sH9aq9VlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing library versions\n",
        "print('Pandas:', pd.__version__)\n",
        "print('Numpy:', np.__version__)\n",
        "print('PyTorch:', torch.__version__)"
      ],
      "metadata": {
        "id": "0AXL-WbwzzVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "AP-TlyoQ0PMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2.  Configuración Inicial y Carga de Datos"
      ],
      "metadata": {
        "id": "ul8b3MgvAEJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf output\n",
        "!rm -rf output.zip\n",
        "!mkdir output"
      ],
      "metadata": {
        "id": "KYNyIrPovvUa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"TRAIN_DIR\": '/content/kaggle/input/df_train.parquet',\n",
        "    \"TEST_DIR\": '/content/kaggle/input/df_test.parquet',\n",
        "    \"SUBMISSION_DIR\": '/content/sample_submission.csv',\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"TARGET_COLUMN\": 'dengue',\n",
        "    \"GROUP_COLUMN\": 'id_bar',\n",
        "    \"WINDOW_SIZE\": 5,\n",
        "    \"HORIZON\": 3,\n",
        "}"
      ],
      "metadata": {
        "id": "tnCzjmsJAGst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración del dispositivo\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "-BCn_ktm0cF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datos\n",
        "train_df = pd.read_parquet(config[\"TRAIN_DIR\"])\n",
        "test_df = pd.read_parquet(config[\"TEST_DIR\"])"
      ],
      "metadata": {
        "id": "R4lWtxiS1gNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.  Preprocesamiento de Datos"
      ],
      "metadata": {
        "id": "eNy7oatl37rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1  Generar Columna fecha\n",
        "Creamos la columna fecha basada en anio y semana, asignando el último día de cada semana como índice."
      ],
      "metadata": {
        "id": "HYmewRQw4EwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar columna 'fecha' (último día de la semana, domingo)\n",
        "def last_day_of_week(year, week):\n",
        "    first_day = datetime.strptime(f'{year} {week} 1', \"%Y %W %w\")\n",
        "    days_ahead = 6 - first_day.weekday()\n",
        "    last_day = first_day + timedelta(days=days_ahead)\n",
        "    return last_day\n",
        "\n",
        "train_df['fecha'] = train_df.apply(lambda row: last_day_of_week(row['anio'], row['semana']), axis=1)\n",
        "test_df['fecha'] = test_df.apply(lambda row: last_day_of_week(row['anio'], row['semana']), axis=1)"
      ],
      "metadata": {
        "id": "DMjHYBgM37EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer 'fecha' como índice\n",
        "train_df.set_index('fecha', inplace=True)\n",
        "test_df.set_index('fecha', inplace=True)"
      ],
      "metadata": {
        "id": "3eyFu1UUZdHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar columnas innecesarias\n",
        "#train_df.drop(['id_bar', 'anio', 'semana'], axis=1, inplace=True)\n",
        "#test_df.drop(['id_bar', 'anio', 'semana'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "gLoH_lheZqEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Particionamos el dataset en entrenamiento hasta el año 2020 y validación el 2021"
      ],
      "metadata": {
        "id": "6N21eCvZdiJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir conjunto de entrenamiento en train y validation\n",
        "train_df_full = train_df.copy()\n",
        "train_df = train_df_full[train_df_full.index.year <= 2020].copy()\n",
        "val_df = train_df_full[train_df_full.index.year >= 2021].copy()"
      ],
      "metadata": {
        "id": "rtoYCc5_ZtWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Selección de Características\n",
        "Definimos las características de entrada, considerando las correlaciones altas entre variables (e.g., lluvia_mean y lluvia_var: 0.82). Para simplificar, usamos todas las características disponibles y dejamos que el modelo aprenda las relaciones."
      ],
      "metadata": {
        "id": "JEsuX1x14WXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir características (excluyendo variables altamente correlacionadas)\n",
        "features = ['ESTRATO', 'area_barrio', 'concentraciones', 'vivienda', 'equipesado', 'sumideros', 'maquina',\n",
        "            'lluvia_mean', 'temperatura_mean', 'temperatura_max']  # Selección basada en correlaciones\n",
        "target = 'dengue'"
      ],
      "metadata": {
        "id": "_0b0rdvB4cBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Normalización\n"
      ],
      "metadata": {
        "id": "KtgONUzG4fZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizamos las características y el objetivo usando StandardScaler. Identificamos las características numéricas (excluyendo id, id_bar y dengue):\n",
        "\n",
        "Características: ESTRATO, area_barrio, concentraciones, vivienda, equipesado, sumideros, maquina, lluvia_mean, lluvia_var, lluvia_max, lluvia_min, temperatura_mean, temperatura_var, temperatura_max, temperatura_min. Ajustamos escaladores por separado para características y objetivo:"
      ],
      "metadata": {
        "id": "3kTHM_xFSOuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se excluyeron variables como lluvia_var, lluvia_max, temperatura_var, y temperatura_min debido a sus altas correlaciones (e.g., lluvia_var y lluvia_mean: 0.82), para reducir redundancia y mejorar la estabilidad de los modelos."
      ],
      "metadata": {
        "id": "oJpWlJ8kZ9qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalización\n",
        "scaler_features = StandardScaler()\n",
        "# scaler_features = MinMaxScaler()\n",
        "train_df[features] = scaler_features.fit_transform(train_df[features])\n",
        "val_df[features] = scaler_features.transform(val_df[features])\n",
        "test_df[features] = scaler_features.transform(test_df[features])\n",
        "\n",
        "# scaler_target = StandardScaler()\n",
        "scaler_target = MinMaxScaler()\n",
        "train_df[target] = scaler_target.fit_transform(train_df[[target]])\n",
        "val_df[target] = scaler_target.transform(val_df[[target]])"
      ],
      "metadata": {
        "id": "g9SU3JzO4jua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standard scaler para las features y el Minmax scaler para el target nos da los mejores resultados."
      ],
      "metadata": {
        "id": "cTRaQFgq2cSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Crear Secuencias para Series Temporales\n",
        "Para predecir con 3 semanas de anticipación, usamos una ventana de 5 semanas (window_size=5) y un horizonte de 3 semanas (horizon=3)."
      ],
      "metadata": {
        "id": "uea5HBGF5y9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(df: pd.DataFrame,\n",
        "                     window_size: int,\n",
        "                     horizon: int,\n",
        "                     features: List[str],\n",
        "                     target: str,\n",
        "                     group_column: str) -> Tuple[List[np.ndarray], List[float]]:\n",
        "    \"\"\"\n",
        "    Creates sequences and labels for time series forecasting.\n",
        "\n",
        "    Args:\n",
        "        df: The input DataFrame containing the time series data.\n",
        "        window_size: The size of the rolling window.\n",
        "        horizon: The forecasting horizon.\n",
        "        features: A list of column names representing the input features.\n",
        "        target: The column name representing the target variable.\n",
        "        group_column: The column name to group the data by (e.g., 'id_bar').\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the sequences (a list of NumPy arrays) and the labels (a list of floats).\n",
        "    \"\"\"\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    groups = df.groupby(group_column)\n",
        "    for _, group in groups:\n",
        "        group = group.sort_index()\n",
        "        for i in range(len(group) - window_size - horizon + 1):\n",
        "            X = group.iloc[i:i + window_size][features].values\n",
        "            y = group.iloc[i + window_size + horizon - 1][target]\n",
        "            sequences.append(X)\n",
        "            labels.append(y)\n",
        "    return sequences, labels\n",
        "\n",
        "train_sequences, train_labels = create_sequences(train_df, config[\"WINDOW_SIZE\"], config[\"HORIZON\"], features, target, config[\"GROUP_COLUMN\"])\n",
        "val_sequences, val_labels = create_sequences(val_df, config[\"WINDOW_SIZE\"], config[\"HORIZON\"], features, target, config[\"GROUP_COLUMN\"])"
      ],
      "metadata": {
        "id": "EuEmBfZ-56bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Dataset y DataLoader\n",
        "Creamos un Dataset personalizado y dividimos en entrenamiento y validación."
      ],
      "metadata": {
        "id": "QoVFGjmM5_bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DengueDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom PyTorch Dataset for Dengue forecasting.\n",
        "\n",
        "    Args:\n",
        "        sequences: A list of NumPy arrays representing the input sequences.\n",
        "        labels: A list of floats representing the corresponding target values.\n",
        "    \"\"\"\n",
        "    def __init__(self, sequences: List[np.ndarray], labels: List[float]):\n",
        "        \"\"\"Initializes the DengueDataset with sequences and labels.\"\"\"\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns the length of the dataset.\"\"\"\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"\n",
        "        Returns the input sequence and label for the given index.\n",
        "\n",
        "        Args:\n",
        "            idx: The index of the item to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            A tuple containing the input sequence (as a PyTorch tensor)\n",
        "            and the corresponding label (as a PyTorch tensor).\n",
        "        \"\"\"\n",
        "        X = self.sequences[idx]\n",
        "        y = self.labels[idx]\n",
        "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "EM4JWY_o6EYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DengueTestDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A custom PyTorch Dataset for Dengue forecasting for test data.\n",
        "\n",
        "    Args:\n",
        "        sequences: A list of NumPy arrays representing the input sequences for test data.\n",
        "    \"\"\"\n",
        "    def __init__(self, sequences: List[np.ndarray]):\n",
        "        \"\"\"Initializes the DengueTestDataset with sequences.\"\"\"\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Returns the length of the dataset.\"\"\"\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Returns the input sequence for the given index.\n",
        "\n",
        "        Args:\n",
        "            idx: The index of the item to retrieve.\n",
        "\n",
        "        Returns:\n",
        "            The input sequence (as a PyTorch tensor).\n",
        "        \"\"\"\n",
        "        X = self.sequences[idx]\n",
        "        return torch.tensor(X, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "EPVVoaE_diEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DengueDataset(train_sequences, train_labels)\n",
        "val_dataset = DengueDataset(val_sequences, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=False)"
      ],
      "metadata": {
        "id": "6_RtD605chV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4.  Implementación de Modelos"
      ],
      "metadata": {
        "id": "VbrI6Wbo6ljL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Modelo MLP\n",
        "Un Perceptrón Multicapa que aplana las secuencias."
      ],
      "metadata": {
        "id": "dhYPwPA66pMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A Multilayer Perceptron (MLP) model for Dengue forecasting.\n",
        "\n",
        "    Args:\n",
        "        input_dim: The dimensionality of the input features.\n",
        "        hidden_dim: The dimensionality of the hidden layers.\n",
        "        layer_dim: The number of hidden layers.\n",
        "        output_dim: The dimensionality of the output (usually 1 for regression).\n",
        "        dropout_rate: The dropout rate to apply between layers (default: 0.2).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, layer_dim:int, output_dim: int, dropout_rate: float = 0.2):\n",
        "        \"\"\"Initializes the MLPModel with specified dimensions and dropout rate.\"\"\"\n",
        "        super(MLPModel, self).__init__()\n",
        "        layers = []\n",
        "        layers.append(nn.Linear(input_dim, hidden_dim))\n",
        "        layers.append(nn.ReLU())\n",
        "        layers.append(nn.Dropout(dropout_rate))\n",
        "        for _ in range(layer_dim - 1):\n",
        "            layers.append(nn.Linear(hidden_dim, hidden_dim))\n",
        "            layers.append(nn.ReLU())\n",
        "            layers.append(nn.Dropout(dropout_rate))\n",
        "\n",
        "        layers.append(nn.Linear(hidden_dim, output_dim))\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the MLP model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of shape (batch_size, sequence_length, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of shape (batch_size, output_dim).\n",
        "        \"\"\"\n",
        "        # batch_size = x.size(0)\n",
        "        # x = x.view(batch_size, -1)\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "KXQl0Ajm6sIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Modelo CNN para Series Temporales\n",
        "Una CNN 1D adaptada a series temporales."
      ],
      "metadata": {
        "id": "YfsmgmEq7H1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A 1D Convolutional Neural Network (CNN) model for Dengue forecasting.\n",
        "\n",
        "    Args:\n",
        "        input_dim: The dimensionality of the input features.\n",
        "        hidden_dim: The dimensionality of the hidden layers.\n",
        "        output_dim: The dimensionality of the output (usually 1 for regression).\n",
        "        dropout_rate: The dropout rate to apply between layers (default: 0.2).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int, dropout_rate: float = 0.2):\n",
        "        \"\"\"Initializes the CNNModel with specified dimensions and dropout rate.\"\"\"\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the CNN model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of shape (batch_size, sequence_length, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of shape (batch_size, output_dim).\n",
        "        \"\"\"\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, window_size)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x).squeeze(-1) # La salida es (batch_size, hidden_dim)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "r0LS148fv7q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Modelo RNN Básico\n",
        "Implementación proporcionada con estados iniciales definidos."
      ],
      "metadata": {
        "id": "RXjpZx3m7M0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A basic Recurrent Neural Network (RNN) model for Dengue forecasting.\n",
        "\n",
        "    Args:\n",
        "        input_dim: The dimensionality of the input features.\n",
        "        hidden_dim: The dimensionality of the hidden state.\n",
        "        layer_dim: The number of RNN layers.\n",
        "        output_dim: The dimensionality of the output (usually 1 for regression).\n",
        "        dropout_rate: The dropout rate to apply between layers (default: 0.2).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, layer_dim: int, output_dim: int, dropout_rate: float = 0.2):\n",
        "        \"\"\"Initializes the RNNModel with specified dimensions and dropout rate.\"\"\"\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the RNN model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of shape (batch_size, sequence_length, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of shape (batch_size, output_dim).\n",
        "        \"\"\"\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "uXl5Db3cxCnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Modelo LSTM"
      ],
      "metadata": {
        "id": "91i05YyF7RhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A Long Short-Term Memory (LSTM) model for Dengue forecasting.\n",
        "\n",
        "    Args:\n",
        "        input_dim: The dimensionality of the input features.\n",
        "        hidden_dim: The dimensionality of the hidden state.\n",
        "        layer_dim: The number of LSTM layers.\n",
        "        output_dim: The dimensionality of the output (usually 1 for regression).\n",
        "        dropout_rate: The dropout rate to apply between layers (default: 0.2).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, layer_dim: int, output_dim: int, dropout_rate: float = 0.2):\n",
        "        \"\"\"Initializes the LSTMModel with specified dimensions and dropout rate.\"\"\"\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the LSTM model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of shape (batch_size, sequence_length, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of shape (batch_size, output_dim).\n",
        "        \"\"\"\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1mipmz3M7V7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.5 Modelo GRU"
      ],
      "metadata": {
        "id": "EAtxxjkp7awI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    \"\"\"\n",
        "    A Gated Recurrent Unit (GRU) model for Dengue forecasting.\n",
        "\n",
        "    Args:\n",
        "        input_dim: The dimensionality of the input features.\n",
        "        hidden_dim: The dimensionality of the hidden state.\n",
        "        layer_dim: The number of GRU layers.\n",
        "        output_dim: The dimensionality of the output (usually 1 for regression).\n",
        "        dropout_rate: The dropout rate to apply between layers (default: 0.2).\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden_dim: int, layer_dim: int, output_dim: int, dropout_rate: float = 0.2):\n",
        "        \"\"\"Initializes the GRUModel with specified dimensions and dropout rate.\"\"\"\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Performs a forward pass through the GRU model.\n",
        "\n",
        "        Args:\n",
        "            x: The input tensor of shape (batch_size, sequence_length, input_dim).\n",
        "\n",
        "        Returns:\n",
        "            The output tensor of shape (batch_size, output_dim).\n",
        "        \"\"\"\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "zIYK3qdt7c6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Entrenamiento y Evaluación"
      ],
      "metadata": {
        "id": "uTEzHGf97gyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.1 Función de Entrenamiento"
      ],
      "metadata": {
        "id": "__VP-McM7jYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model: nn.Module,\n",
        "                train_loader: DataLoader,\n",
        "                epochs: int,\n",
        "                optimizer: optim.Optimizer,\n",
        "                criterion: nn.Module,\n",
        "                device: torch.device,\n",
        "                val_loader: DataLoader = None,\n",
        "                patience: int = 10) -> Tuple[List[float], List[float]]:\n",
        "    \"\"\"\n",
        "    Trains a PyTorch model and returns the training and validation losses.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to train.\n",
        "        train_loader: The DataLoader for the training data.\n",
        "        val_loader: The DataLoader for the validation data. If None, no validation is performed.\n",
        "        epochs: The number of epochs to train for.\n",
        "        optimizer: The optimizer to use.\n",
        "        criterion: The loss function to use.\n",
        "        device: The device to train on (e.g., 'cpu' or 'cuda').\n",
        "        patience: The number of epochs to wait for improvement before stopping. Default is 10.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the training losses and validation losses.\n",
        "    \"\"\"\n",
        "\n",
        "    model.to(device)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    best_loss = float('inf')\n",
        "    epochs_without_improvement = 0\n",
        "    best_model = None\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        if val_loader is None:\n",
        "          if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}')\n",
        "          if train_loss < best_loss:\n",
        "            best_loss = train_loss\n",
        "            best_model = copy.deepcopy(model)\n",
        "            epochs_without_improvement = 0\n",
        "          else:\n",
        "            epochs_without_improvement += 1\n",
        "            if epochs_without_improvement >= patience:\n",
        "              print(f'Early stopping at epoch {epoch+1}')\n",
        "              break\n",
        "        else:\n",
        "          if (epoch + 1) % 10 == 0:\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "          model.eval()\n",
        "          val_loss = 0\n",
        "          with torch.no_grad():\n",
        "              for X, y in val_loader:\n",
        "                  X, y = X.to(device), y.to(device)\n",
        "                  output = model(X)\n",
        "                  loss = criterion(output, y)\n",
        "                  val_loss += loss.item()\n",
        "          val_loss /= len(val_loader)\n",
        "          val_losses.append(val_loss)\n",
        "          if val_loss < best_loss:\n",
        "              best_loss = val_loss\n",
        "              best_model = copy.deepcopy(model)\n",
        "              epochs_without_improvement = 0\n",
        "          else:\n",
        "              epochs_without_improvement += 1\n",
        "              if epochs_without_improvement >= patience:\n",
        "                  print(f'Early stopping at epoch {epoch+1}')\n",
        "                  break\n",
        "\n",
        "    model = best_model\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "5u1w28uH7lcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.2 Función de Evaluación"
      ],
      "metadata": {
        "id": "PxuM2Xd47vji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model: torch.nn.Module,\n",
        "                   val_loader: torch.utils.data.DataLoader,\n",
        "                   device: torch.device,\n",
        "                   scaler_target: MinMaxScaler | StandardScaler) -> Tuple[float, float, float, np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Evaluates a PyTorch model on a validation set and returns evaluation metrics.\n",
        "\n",
        "    Args:\n",
        "        model: The PyTorch model to evaluate.\n",
        "        val_loader: The DataLoader for the validation data.\n",
        "        device: The device to evaluate on (e.g., 'cpu' or 'cuda').\n",
        "        scaler_target: The scaler used to normalize the target variable.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing the mean absolute error (MAE), mean squared error (MSE),\n",
        "        root mean squared error (RMSE), predicted values, and actual values.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            output = model(X)\n",
        "            predictions.append(output.cpu().numpy())\n",
        "            actuals.append(y.cpu().numpy())\n",
        "    predictions = np.concatenate(predictions)\n",
        "    actuals = np.concatenate(actuals)\n",
        "    predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "    actuals = scaler_target.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
        "    return mae, mse, rmse, predictions, actuals"
      ],
      "metadata": {
        "id": "NacHAVl_71M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.3 Gráficos\n",
        "Generamos gráficos de pérdidas y predicciones vs reales."
      ],
      "metadata": {
        "id": "UpDtMX208FxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses: List[float], val_losses: List[float]) -> None:\n",
        "    \"\"\"\n",
        "    Generates a plot of training and validation losses.\n",
        "\n",
        "    Args:\n",
        "        train_losses: A list of training losses for each epoch.\n",
        "        val_losses: A list of validation losses for each epoch.\n",
        "    \"\"\"\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.show()\n",
        "\n",
        "def plot_predictions(actuals: np.ndarray, predictions: np.ndarray) -> None:\n",
        "    \"\"\"\n",
        "    Generates a plot of actual vs predicted values.\n",
        "\n",
        "    Args:\n",
        "        actuals: A NumPy array of actual values.\n",
        "        predictions: A NumPy array of predicted values.\n",
        "    \"\"\"\n",
        "    plt.plot(actuals, label='Actual')\n",
        "    plt.plot(predictions, label='Predicted')\n",
        "    plt.xlabel('Sample')\n",
        "    plt.ylabel('Dengue Cases')\n",
        "    plt.legend()\n",
        "    plt.title('Actual vs Predicted Values')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kDnTvwsS8FMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.4 Entrenar Modelos con optimización bayesiana"
      ],
      "metadata": {
        "id": "TcSszRDU749v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bayesian_optimization(model_class: Type[nn.Module],\n",
        "                           train_loader: DataLoader,\n",
        "                           val_loader: DataLoader,\n",
        "                           space: Dict[str, Any],\n",
        "                           max_evals: int = 50,\n",
        "                           device: Union[str, torch.device] = 'cpu') -> Dict[str, Any]:\n",
        "\n",
        "    \"\"\"\n",
        "    Performs Bayesian optimization for hyperparameter tuning.\n",
        "\n",
        "    Args:\n",
        "        model_class: The PyTorch model class to optimize.\n",
        "        train_loader: The DataLoader for the training data.\n",
        "        val_loader: The DataLoader for the validation data.\n",
        "        space: The search space for hyperparameters.\n",
        "        max_evals: The maximum number of evaluations\n",
        "        device: The device to train on (e.g., 'cpu' or 'cuda'). Default is 'cpu'.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary containing the best hyperparameters and the best model.\n",
        "    \"\"\"\n",
        "\n",
        "    def objective(params):\n",
        "        \"\"\"\n",
        "        Objective function for Bayesian optimization.\n",
        "\n",
        "        Args:\n",
        "            params: The hyperparameters to optimize.\n",
        "\n",
        "        Returns:\n",
        "            The negative validation loss.\n",
        "        \"\"\"\n",
        "\n",
        "        # Adjust logic depending on model type CNN or MLP:\n",
        "        if model_class == CNNModel:\n",
        "            model = model_class(input_dim=len(features),\n",
        "                             hidden_dim=params['hidden_dim'], output_dim=1, dropout_rate=params['dropout_rate'])\n",
        "        else:\n",
        "            model = model_class(input_dim=len(features),\n",
        "                             hidden_dim=params['hidden_dim'], layer_dim=params['layer_dim'], output_dim=1, dropout_rate=params['dropout_rate'])\n",
        "\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = params['optimizer'](model.parameters(), lr=params['lr'])\n",
        "        train_losses, val_losses = train_model(model=model, train_loader=train_loader, val_loader=val_loader, epochs=params['epochs'], optimizer=optimizer, criterion=criterion, device=device)\n",
        "\n",
        "        return {'loss': val_losses[-1], 'status': STATUS_OK, 'model': model}\n",
        "\n",
        "    trials = Trials()\n",
        "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
        "\n",
        "    # Get the best model from the trials\n",
        "    best_trial = trials.best_trial\n",
        "    best_model = best_trial['result']['model'] # Get the best model\n",
        "\n",
        "\n",
        "    return {'best_params': best, 'best_model': best_model}"
      ],
      "metadata": {
        "id": "qTvtQcr9ddwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hiperparametros del espacio de búsqueda"
      ],
      "metadata": {
        "id": "e07rplY-Adsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_choices = [50, 100, 150, 200]\n",
        "hidden_dim_choices = [32, 64, 128, 256]\n",
        "optimizer_choices = [optim.Adam, optim.RMSprop, optim.SGD]\n",
        "layer_dim_choices = [1, 2, 3]\n",
        "max_evals_bayesian_search = 50"
      ],
      "metadata": {
        "id": "l32P0EC2qHjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "space = {\n",
        "    'epochs': hp.choice('epochs', epochs_choices),\n",
        "    'lr': hp.loguniform('lr', np.log(0.001), np.log(0.1)),\n",
        "    'optimizer': hp.choice('optimizer', optimizer_choices),\n",
        "    'hidden_dim': hp.choice('hidden_dim', hidden_dim_choices),\n",
        "    'layer_dim': hp.choice('layer_dim', layer_dim_choices),\n",
        "    'dropout_rate': hp.uniform('dropout_rate', 0, 0.5)\n",
        "}"
      ],
      "metadata": {
        "id": "GQWy3jcjNvgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4.1 MLP"
      ],
      "metadata": {
        "id": "EfJFd6rXd1hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best_model = bayesian_optimization(MLPModel, train_loader, val_loader, space, max_evals=max_evals_bayesian_search, device=DEVICE)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "k1972LeeeOJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4.1 CNN"
      ],
      "metadata": {
        "id": "T6G7tnlid46C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best_model = bayesian_optimization(CNNModel, train_loader, val_loader, space, max_evals=max_evals_bayesian_search, device=DEVICE)"
      ],
      "metadata": {
        "id": "azWrt01efI-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4.1 RNN"
      ],
      "metadata": {
        "id": "BV65Z64qd6sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best_model = bayesian_optimization(RNNModel, train_loader, val_loader, space, max_evals=max_evals_bayesian_search, device=DEVICE)"
      ],
      "metadata": {
        "id": "C-aXiElTfMZr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4.1 LSTM"
      ],
      "metadata": {
        "id": "Ka4QI_0Wd8Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best_model = bayesian_optimization(LSTMModel, train_loader, val_loader, space, max_evals=max_evals_bayesian_search, device=DEVICE)"
      ],
      "metadata": {
        "id": "wgCEykClfPNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5.4.1 GRU"
      ],
      "metadata": {
        "id": "eHT0fwu_d-X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# best_model = bayesian_optimization(GRUModel, train_loader, val_loader, space, max_evals=max_evals_bayesian_search, device=DEVICE)"
      ],
      "metadata": {
        "id": "yXkRx_iffSDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mejores hiperpárametros"
      ],
      "metadata": {
        "id": "i_FI_b02AmAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Mejores parámetros:\")\n",
        "print(f\"dropout_rate: {best_model['best_params']['dropout_rate']}\")\n",
        "print(f\"epochs: {epochs_choices[best_model['best_params']['epochs']]}\")\n",
        "print(f\"hidden_dim: {hidden_dim_choices[best_model['best_params']['hidden_dim']]}\")\n",
        "print(f\"lr: {best_model['best_params']['lr']}\")\n",
        "print(f\"optimizer: {optimizer_choices[best_model['best_params']['optimizer']]}\")\n",
        "if 'layer_dim' in best_model['best_params']:\n",
        "  print(f\"layer_dim: {layer_dim_choices[best_model['best_params']['layer_dim']]}\")\n",
        "print(\"Mejor modelo:\", best_model['best_model'])"
      ],
      "metadata": {
        "id": "BfbFkN59Mks0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('output/params.txt', 'w') as f:\n",
        "    print(\"Mejores parámetros:\", file=f)\n",
        "    print(f\"dropout_rate: {best_model['best_params']['dropout_rate']}\", file=f)\n",
        "    print(f\"epochs: {epochs_choices[best_model['best_params']['epochs']]}\", file=f)\n",
        "    print(f\"hidden_dim: {hidden_dim_choices[best_model['best_params']['hidden_dim']]}\", file=f)\n",
        "    print(f\"lr: {best_model['best_params']['lr']}\", file=f)\n",
        "    print(f\"optimizer: {optimizer_choices[best_model['best_params']['optimizer']]}\", file=f)\n",
        "    if 'layer_dim' in best_model['best_params']:\n",
        "        print(f\"layer_dim: {layer_dim_choices[best_model['best_params']['layer_dim']]}\", file=f)\n",
        "    print(\"Mejor modelo:\", best_model['best_model'], file=f)"
      ],
      "metadata": {
        "id": "pIw8QAxJuzlD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Predicción en el Test Set"
      ],
      "metadata": {
        "id": "LVdjsQW78pIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.1 Re entrenar modelo final"
      ],
      "metadata": {
        "id": "sgoAdOEI83Vp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos una copia del modelo para re entrenar con todos los datos disponibles."
      ],
      "metadata": {
        "id": "wPztJe2txYd7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_trained = copy.deepcopy(best_model['best_model']).to(DEVICE)"
      ],
      "metadata": {
        "id": "YDWNB-DX7m9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preparamos los datos para generar la secuencia como la espera el modelo, creando el dataset y el dataloader"
      ],
      "metadata": {
        "id": "bnAtXBL8xgTo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.\n",
        "train_df_full[features] = scaler_features.transform(train_df_full[features])\n",
        "train_df_full[target] = scaler_target.transform(train_df_full[[target]])\n",
        "\n",
        "# 2.\n",
        "train_sequences_full, train_labels_full = create_sequences(train_df_full, config[\"WINDOW_SIZE\"], config[\"HORIZON\"], features, target, config[\"GROUP_COLUMN\"])\n",
        "\n",
        "# 3.\n",
        "train_dataset_full = DengueDataset(train_sequences_full, train_labels_full)\n",
        "train_loader_full = DataLoader(train_dataset_full, batch_size=config[\"BATCH_SIZE\"], shuffle=False)"
      ],
      "metadata": {
        "id": "xXizf831_oF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entrenamos nuevamente el modelo usando los mejores hiperparametros y basandonos en el mejor modelo"
      ],
      "metadata": {
        "id": "30fiv9RzxtoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n🔁 Reentrenando con todo el dataset (train + val)...\\n\")\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optimizer_choices[best_model['best_params']['optimizer']](best_model_trained.parameters(), lr=best_model['best_params']['lr'])\n",
        "train_losses, val_losses = train_model(best_model_trained, train_loader_full,  epochs=epochs_choices[best_model['best_params']['epochs']], optimizer=optimizer, criterion=criterion, device=DEVICE)"
      ],
      "metadata": {
        "id": "mwTir67aBfXS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.2 Crear Secuencias para Test\n",
        "Combinamos train y test para obtener las semanas previas necesarias."
      ],
      "metadata": {
        "id": "etAZbHts8uo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([train_df_full, test_df], sort=False)\n",
        "combined_df = combined_df.sort_values(by=['id_bar', 'fecha'])"
      ],
      "metadata": {
        "id": "ynhDKcV98ys6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences = []\n",
        "ids = []\n",
        "\n",
        "for idx, row in test_df.iterrows():\n",
        "    id_bar = row['id_bar']\n",
        "    fecha = row.name\n",
        "    prev_dates = combined_df[(combined_df['id_bar'] == id_bar) & (combined_df.index < fecha)].tail(config[\"WINDOW_SIZE\"])\n",
        "    if len(prev_dates) == config[\"WINDOW_SIZE\"]:\n",
        "        seq = prev_dates[features].values\n",
        "        test_sequences.append(seq)\n",
        "        ids.append(row['id'])\n"
      ],
      "metadata": {
        "id": "GgynaqcgRe0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sequences = np.array(test_sequences)\n",
        "test_tensor = torch.tensor(test_sequences, dtype=torch.float32).to(DEVICE)"
      ],
      "metadata": {
        "id": "h6XEwrRWQWi3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.  Submission"
      ],
      "metadata": {
        "id": "HJO3iLJO0klb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero preparamos el dataset de prueba"
      ],
      "metadata": {
        "id": "7QfmOEyM1hZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = DengueTestDataset(test_sequences)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "tDY0iw4X856w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "generamos la predicción con el mejor modelo re-entrenado"
      ],
      "metadata": {
        "id": "pg61SRq81jxv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forecasting(modelo, dataloader):\n",
        "    modelo.eval()\n",
        "    predictions = []\n",
        "    with torch.no_grad():\n",
        "        for X in dataloader:\n",
        "            X = X.to(DEVICE)\n",
        "            output = modelo(X)\n",
        "            predictions.append(output.cpu().numpy())\n",
        "\n",
        "    predictions = np.concatenate(predictions)\n",
        "    predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "\n",
        "    return predictions"
      ],
      "metadata": {
        "id": "XuHrD3NkLIST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = forecasting(best_model_trained, test_loader)\n",
        "\n",
        "df_submission = pd.DataFrame({'id': ids, 'dengue': predictions})\n",
        "df_submission.to_csv('output/submission_retrain.csv', index=False)\n",
        "print(f'Submission guardado en submission_retrain.csv, con {len(df_submission)} predicciones.')"
      ],
      "metadata": {
        "id": "AzG1Xk47b3Zw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generamos la predicción con el mejor modelo sin re-entrenar"
      ],
      "metadata": {
        "id": "PeYQdy8n1ysI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = forecasting(best_model['best_model'], test_loader)\n",
        "\n",
        "# Prepare submission\n",
        "df_submission = pd.DataFrame({'id': ids, 'dengue': predictions})\n",
        "df_submission.to_csv('output/submission.csv', index=False)\n",
        "print(f'Submission guardado en submission.csv, con {len(df_submission)} predicciones.')"
      ],
      "metadata": {
        "id": "upekEBDp16LH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8. Guardar modelo"
      ],
      "metadata": {
        "id": "YRIWpxjD1ZVd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(best_model_trained.state_dict(), 'output/best_model_retrained.pth')\n",
        "torch.save(best_model['best_model'].state_dict(), 'output/best_model.pth')"
      ],
      "metadata": {
        "id": "RxEv4ryo1ccP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('output/model_summary.txt', 'w') as f:\n",
        "    print(\"Model Summary:\", file=f)\n",
        "    print(summary(best_model_trained, (1, len(features))), file=f)"
      ],
      "metadata": {
        "id": "_Dv3Akvr4oVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_!zip -r output.zip output"
      ],
      "metadata": {
        "id": "rLsFcjeJwLuu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}