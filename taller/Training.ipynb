{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NickEsColR/MachineLearningV/blob/train/taller/Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equipo**\n",
        "\n",
        "- Nicolás Colmenares\n",
        "\n",
        "- Carlos Martinez"
      ],
      "metadata": {
        "id": "Ckrr43Eb7-jD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Situación:**\n",
        "Una ciudad enfrenta un aumento significativo de casos de dengue, con una tasa de incidencia que supera el promedio nacional.\n",
        "La anticipación de brotes es crucial para implementar medidas preventivas y reducir la propagación de la enfermedad.\n",
        "\n",
        "**Objetivo:**\n",
        "Desarrollar un modelo predictivo utilizando redes neuronales para pronosticar futuros brotes de dengue en cada barrio de la ciudad.\n",
        "Utilizar una base de datos histórica de casos de dengue desde 2015 hasta 2022 para entrenar el modelo.\n",
        "Anticiparse a los brotes con al menos 3 semanas de anticipación.\n",
        "\n",
        "**Finalidad:**\n",
        "Permitir a las autoridades de salud pública tomar acciones oportunas, como:\n",
        "Preparar a las instituciones prestadoras de salud (IPS).\n",
        "Gestionar recursos (carros fumigadores, limpieza de sumideros).\n",
        "Capacitar a la comunidad."
      ],
      "metadata": {
        "id": "zQ9T88GEx2Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Redes Neuronales Tradicinales (MLP)\n",
        "2. Red Convolucional (CNN) adaptada a series temporales\n",
        "3. Red Neuronal Recurrente (RNN) básica.\n",
        "4. Modelo con LSTMs\n",
        "5. Modelo con GRUs"
      ],
      "metadata": {
        "id": "dVtVgKGCyXfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diccionario"
      ],
      "metadata": {
        "id": "R3PIeauF9c56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.parquet - El conjunto de datos de entrenamiento\n",
        "test.parquet - El conjunto de datos de prueba\n",
        "sample_submission.csv - un ejemplo de un archivo a someter en la competencia"
      ],
      "metadata": {
        "id": "uwGcqMDn-AoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Variable**         | **Descripción**                                                                                      |\n",
        "|-----------------------|------------------------------------------------------------------------------------------------------|\n",
        "| id_bar               | identificador único del barrio                                                                      |\n",
        "| anio                 | Año de ocurrencia                                                                                   |\n",
        "| semana               | Semana de ocurrencia                                                                               |\n",
        "| Estrato              | Estrato socioeconómico del barrio                                                                   |\n",
        "| area_barrio          | Área del barrio en km²                                                                             |\n",
        "| dengue               | Conteo de casos de dengue                                                                          |\n",
        "| concentraciones      | Cantidad de visitas e intervención a lugares de concentración humana (Instituciones)                |\n",
        "| vivienda             | Conteo de las visitas a viviendas a revisión y control de criaderos                                 |\n",
        "| equipesado           | Conteo de las fumigaciones con Maquinaria Pesada                                                   |\n",
        "| sumideros            | Conteo de las intervenciones a los sumideros                                                       |\n",
        "| maquina              | Conteo de las fumigaciones con motomochila                                                         |\n",
        "| lluvia_mean          | Lluvia promedio en la semana i                                                                     |\n",
        "| lluvia_var           | Varianza de la lluvia en la semana i                                                               |\n",
        "| lluvia_max           | Lluvia máxima en la semana i                                                                       |\n",
        "| lluvia_min           | Lluvia mínima en la semana i                                                                       |\n",
        "| temperatura_mean     | Temperatura promedio en la semana i                                                                |\n",
        "| temperatura_var      | Varianza de la temperatura en la semana i                                                          |\n",
        "| temperatura_max      | Temperatura máxima en la semana i                                                                  |\n",
        "| temperatura_min      | Temperatura mínima en la semana i                                                                  |\n"
      ],
      "metadata": {
        "id": "wKLm80Huf2vY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Configuraciones de Colab"
      ],
      "metadata": {
        "id": "l9U_uqnvvYtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mover Kaggle.json a la ubicación correcta después de subirlo"
      ],
      "metadata": {
        "id": "zi1MbjLG81SQ"
      }
    },
    {
      "source": [
        "#Estas líneas son comandos de shell que se ejecutan dentro del Jupyter notebook. Se usan para configurar las credenciales de la API de Kaggle, que son necesarias para descargar conjuntos de datos (datasets) desde Kaggle.\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FkW9-t478uLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/kaggle/output\n",
        "!rm -rf /content/kaggle/input"
      ],
      "metadata": {
        "id": "-YMKmQruoaHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargar dataset de la competencia"
      ],
      "metadata": {
        "id": "VWH9Y2UG9KLD"
      }
    },
    {
      "source": [
        "!kaggle competitions download -c aa-v-2025-i-pronosticos-nn-rnn-cnn"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SnUZl0d-8_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/kaggle/output\n",
        "!mkdir -p /content/kaggle/input"
      ],
      "metadata": {
        "id": "vaLprrm32zGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv aa-v-2025-i-pronosticos-nn-rnn-cnn.zip /content/kaggle/input"
      ],
      "metadata": {
        "id": "s07nq5K1zyuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/kaggle/input/aa-v-2025-i-pronosticos-nn-rnn-cnn.zip -d /content/kaggle/input/"
      ],
      "metadata": {
        "id": "skxqP_oq0VeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/kaggle/input\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "0m2oYgGOxJue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Imports"
      ],
      "metadata": {
        "id": "FiQLr9h-0Cr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta # Importing the required modules datetime and timedelta\n",
        "\n",
        "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
      ],
      "metadata": {
        "id": "MO0sH9aq9VlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing library versions\n",
        "print('Pandas:', pd.__version__)\n",
        "print('Numpy:', np.__version__)\n",
        "print('PyTorch:', torch.__version__)"
      ],
      "metadata": {
        "id": "0AXL-WbwzzVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "AP-TlyoQ0PMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Configuración Inicial y Carga de Datos"
      ],
      "metadata": {
        "id": "ul8b3MgvAEJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"TRAIN_DIR\": '/content/kaggle/input/df_train.parquet',\n",
        "    \"TEST_DIR\": '/content/kaggle/input/df_test.parquet',\n",
        "    \"SUBMISSION_DIR\": '/content/sample_submission.csv',\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"TARGET_COLUMN\": 'dengue',\n",
        "    \"GROUP_COLUMN\": 'id_bar',\n",
        "    \"WINDOW_SIZE\": 5,\n",
        "    \"HORIZON\": 3,\n",
        "}"
      ],
      "metadata": {
        "id": "tnCzjmsJAGst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración del dispositivo\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "-BCn_ktm0cF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datos\n",
        "train_df = pd.read_parquet(config[\"TRAIN_DIR\"])\n",
        "test_df = pd.read_parquet(config[\"TEST_DIR\"])"
      ],
      "metadata": {
        "id": "R4lWtxiS1gNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Preprocesamiento de Datos"
      ],
      "metadata": {
        "id": "eNy7oatl37rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 Generar Columna fecha\n",
        "Creamos la columna fecha basada en anio y semana, asignando el último día de cada semana como índice."
      ],
      "metadata": {
        "id": "HYmewRQw4EwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar columna 'fecha' (último día de la semana, domingo)\n",
        "def last_day_of_week(year, week):\n",
        "    first_day = datetime.strptime(f'{year} {week} 1', \"%Y %W %w\")\n",
        "    days_ahead = 6 - first_day.weekday()\n",
        "    last_day = first_day + timedelta(days=days_ahead)\n",
        "    return last_day\n",
        "\n",
        "train_df['fecha'] = train_df.apply(lambda row: last_day_of_week(row['anio'], row['semana']), axis=1)\n",
        "test_df['fecha'] = test_df.apply(lambda row: last_day_of_week(row['anio'], row['semana']), axis=1)"
      ],
      "metadata": {
        "id": "DMjHYBgM37EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer 'fecha' como índice\n",
        "train_df.set_index('fecha', inplace=True)\n",
        "test_df.set_index('fecha', inplace=True)"
      ],
      "metadata": {
        "id": "3eyFu1UUZdHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar columnas innecesarias\n",
        "#train_df.drop(['id_bar', 'anio', 'semana'], axis=1, inplace=True)\n",
        "#test_df.drop(['id_bar', 'anio', 'semana'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "gLoH_lheZqEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Particionamos el dataset en entrenamiento hasta el año 2020 y validación el 2021"
      ],
      "metadata": {
        "id": "6N21eCvZdiJe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir conjunto de entrenamiento en train y validation\n",
        "train_df_full = train_df.copy()\n",
        "train_df = train_df_full[train_df_full.index.year <= 2020].copy()\n",
        "val_df = train_df_full[train_df_full.index.year >= 2021].copy()"
      ],
      "metadata": {
        "id": "rtoYCc5_ZtWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 Selección de Características\n",
        "Definimos las características de entrada, considerando las correlaciones altas entre variables (e.g., lluvia_mean y lluvia_var: 0.82). Para simplificar, usamos todas las características disponibles y dejamos que el modelo aprenda las relaciones."
      ],
      "metadata": {
        "id": "JEsuX1x14WXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir características (excluyendo variables altamente correlacionadas)\n",
        "features = ['ESTRATO', 'area_barrio', 'concentraciones', 'vivienda', 'equipesado', 'sumideros', 'maquina',\n",
        "            'lluvia_mean', 'temperatura_mean', 'temperatura_max']  # Selección basada en correlaciones\n",
        "target = 'dengue'"
      ],
      "metadata": {
        "id": "_0b0rdvB4cBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3 Normalización\n"
      ],
      "metadata": {
        "id": "KtgONUzG4fZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizamos las características y el objetivo usando StandardScaler. Identificamos las características numéricas (excluyendo id, id_bar y dengue):\n",
        "\n",
        "Características: ESTRATO, area_barrio, concentraciones, vivienda, equipesado, sumideros, maquina, lluvia_mean, lluvia_var, lluvia_max, lluvia_min, temperatura_mean, temperatura_var, temperatura_max, temperatura_min. Ajustamos escaladores por separado para características y objetivo:"
      ],
      "metadata": {
        "id": "3kTHM_xFSOuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se excluyeron variables como lluvia_var, lluvia_max, temperatura_var, y temperatura_min debido a sus altas correlaciones (e.g., lluvia_var y lluvia_mean: 0.82), para reducir redundancia y mejorar la estabilidad de los modelos."
      ],
      "metadata": {
        "id": "oJpWlJ8kZ9qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalización\n",
        "scaler_features = StandardScaler()\n",
        "train_df[features] = scaler_features.fit_transform(train_df[features])\n",
        "val_df[features] = scaler_features.transform(val_df[features])\n",
        "test_df[features] = scaler_features.transform(test_df[features])\n",
        "\n",
        "scaler_target = StandardScaler()\n",
        "train_df[target] = scaler_target.fit_transform(train_df[[target]])\n",
        "val_df[target] = scaler_target.transform(val_df[[target]])"
      ],
      "metadata": {
        "id": "g9SU3JzO4jua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3 Crear Secuencias para Series Temporales\n",
        "Para predecir con 3 semanas de anticipación, usamos una ventana de 5 semanas (window_size=5) y un horizonte de 3 semanas (horizon=3)."
      ],
      "metadata": {
        "id": "uea5HBGF5y9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(df, window_size, horizon, features, target, group_column):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    groups = df.groupby(group_column)\n",
        "    for _, group in groups:\n",
        "        group = group.sort_index()\n",
        "        for i in range(len(group) - window_size - horizon + 1):\n",
        "            X = group.iloc[i:i + window_size][features].values\n",
        "            y = group.iloc[i + window_size + horizon - 1][target]\n",
        "            sequences.append(X)\n",
        "            labels.append(y)\n",
        "    return sequences, labels\n",
        "\n",
        "train_sequences, train_labels = create_sequences(train_df, config[\"WINDOW_SIZE\"], config[\"HORIZON\"], features, target, config[\"GROUP_COLUMN\"])\n",
        "val_sequences, val_labels = create_sequences(val_df, config[\"WINDOW_SIZE\"], config[\"HORIZON\"], features, target, config[\"GROUP_COLUMN\"])"
      ],
      "metadata": {
        "id": "EuEmBfZ-56bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.4 Dataset y DataLoader\n",
        "Creamos un Dataset personalizado y dividimos en entrenamiento y validación."
      ],
      "metadata": {
        "id": "QoVFGjmM5_bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DengueDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.sequences[idx]\n",
        "        y = self.labels[idx]\n",
        "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "EM4JWY_o6EYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DengueTestDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.sequences[idx]\n",
        "        return torch.tensor(X, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "EPVVoaE_diEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DengueDataset(train_sequences, train_labels)\n",
        "val_dataset = DengueDataset(val_sequences, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=False)"
      ],
      "metadata": {
        "id": "6_RtD605chV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Implementación de Modelos"
      ],
      "metadata": {
        "id": "VbrI6Wbo6ljL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.1 Modelo MLP\n",
        "Un Perceptrón Multicapa que aplana las secuencias."
      ],
      "metadata": {
        "id": "dhYPwPA66pMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.2):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "KXQl0Ajm6sIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.2 Modelo CNN para Series Temporales\n",
        "Una CNN 1D adaptada a series temporales."
      ],
      "metadata": {
        "id": "YfsmgmEq7H1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.2):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, window_size)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x).squeeze(-1) # La salida es (batch_size, hidden_dim)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "r0LS148fv7q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.3 Modelo RNN Básico\n",
        "Implementación proporcionada con estados iniciales definidos."
      ],
      "metadata": {
        "id": "RXjpZx3m7M0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_rate=0.2):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "uXl5Db3cxCnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.4 Modelo LSTM"
      ],
      "metadata": {
        "id": "91i05YyF7RhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_rate=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1mipmz3M7V7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.5 Modelo GRU"
      ],
      "metadata": {
        "id": "EAtxxjkp7awI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_rate=0.2):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "zIYK3qdt7c6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Entrenamiento y Evaluación"
      ],
      "metadata": {
        "id": "uTEzHGf97gyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.1 Función de Entrenamiento"
      ],
      "metadata": {
        "id": "__VP-McM7jYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs, optimizer, criterion, device):\n",
        "    model.to(device)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                output = model(X)\n",
        "                loss = criterion(output, y)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:  # Imprimir solo cada 10 épocas\n",
        "          print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "5u1w28uH7lcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.2 Función de Evaluación"
      ],
      "metadata": {
        "id": "PxuM2Xd47vji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, val_loader, device, scaler_target):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            output = model(X)\n",
        "            predictions.append(output.cpu().numpy())\n",
        "            actuals.append(y.cpu().numpy())\n",
        "    predictions = np.concatenate(predictions)\n",
        "    actuals = np.concatenate(actuals)\n",
        "    predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "    actuals = scaler_target.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
        "    return mae, mse, rmse, predictions, actuals"
      ],
      "metadata": {
        "id": "NacHAVl_71M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.3 Gráficos\n",
        "Generamos gráficos de pérdidas y predicciones vs reales."
      ],
      "metadata": {
        "id": "UpDtMX208FxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses, val_losses):\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.show()\n",
        "\n",
        "def plot_predictions(actuals, predictions):\n",
        "    plt.plot(actuals, label='Actual')\n",
        "    plt.plot(predictions, label='Predicted')\n",
        "    plt.xlabel('Sample')\n",
        "    plt.ylabel('Dengue Cases')\n",
        "    plt.legend()\n",
        "    plt.title('Actual vs Predicted Values')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kDnTvwsS8FMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.4 Entrenar Modelos con optimización bayesiana"
      ],
      "metadata": {
        "id": "TcSszRDU749v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bayesian_optimization(model_class, train_dataset, val_dataset, space, max_evals=50, device='cpu'):\n",
        "    \"\"\"\n",
        "    Realiza la optimización bayesiana de los hiperparámetros de un modelo.\n",
        "\n",
        "    Args:\n",
        "        model_class: La clase del modelo a optimizar.\n",
        "        train_dataset: El conjunto de datos de entrenamiento.\n",
        "        val_dataset: El conjunto de datos de validación.\n",
        "        space: El espacio de búsqueda de hiperparámetros.\n",
        "        max_evals: El número máximo de evaluaciones.\n",
        "        device: El dispositivo a utilizar ('cpu' o 'cuda').\n",
        "\n",
        "    Returns:\n",
        "        Un diccionario con los mejores hiperparámetros y el mejor modelo.\n",
        "    \"\"\"\n",
        "\n",
        "    def objective(params):\n",
        "        \"\"\"\n",
        "        Función objetivo para la optimización bayesiana.\n",
        "        \"\"\"\n",
        "\n",
        "        # Ajustar lógica si el modelo es CNN o MLP:\n",
        "        if model_class == MLPModel:\n",
        "            model = model_class(input_dim=config[\"WINDOW_SIZE\"] * len(features),\n",
        "                             hidden_dim=params['hidden_dim'], output_dim=1, dropout_rate=params['dropout_rate'])\n",
        "        elif model_class == CNNModel:\n",
        "            model = model_class(input_dim=len(features),\n",
        "                             hidden_dim=params['hidden_dim'], output_dim=1, dropout_rate=params['dropout_rate'])\n",
        "        else:\n",
        "            model = model_class(input_dim=len(features),\n",
        "                             hidden_dim=params['hidden_dim'], layer_dim=params['layer_dim'], output_dim=1, dropout_rate=params['dropout_rate'])\n",
        "\n",
        "        criterion = nn.MSELoss()\n",
        "        optimizer = params['optimizer'](model.parameters(), lr=params['lr'])\n",
        "        train_losses, val_losses = train_model(model, train_dataset, val_dataset, epochs=params['epochs'], optimizer=optimizer, criterion=criterion, device=device)\n",
        "\n",
        "        return {'loss': val_losses[-1], 'status': STATUS_OK, 'model': model}\n",
        "\n",
        "    trials = Trials()\n",
        "    best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=max_evals, trials=trials)\n",
        "\n",
        "    # Get the best model from the trials\n",
        "    best_trial = trials.best_trial\n",
        "    best_model = best_trial['result']['model'] # Get the best model\n",
        "\n",
        "\n",
        "    return {'best_params': best, 'best_model': best_model}"
      ],
      "metadata": {
        "id": "qTvtQcr9ddwW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hiperparametros del espacio de búsqueda"
      ],
      "metadata": {
        "id": "e07rplY-Adsq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_choices = [50, 100, 150]\n",
        "hidden_dim_choices = [32, 64, 128]\n",
        "optimizer_choices = [optim.Adam, optim.RMSprop, optim.SGD]\n",
        "layer_dim_choices = [1, 2, 3]"
      ],
      "metadata": {
        "id": "l32P0EC2qHjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4.1 MLP"
      ],
      "metadata": {
        "id": "EfJFd6rXd1hc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define el espacio de búsqueda para MLP\n",
        "space_mlp = {\n",
        "    'epochs': hp.choice('epochs', epochs_choices),\n",
        "    'lr': hp.loguniform('lr', np.log(0.001), np.log(0.1)),\n",
        "    'optimizer': hp.choice('optimizer', optimizer_choices),\n",
        "    'hidden_dim': hp.choice('hidden_dim', hidden_dim_choices),\n",
        "    'dropout_rate': hp.uniform('dropout_rate', 0, 0.5)\n",
        "}\n",
        "\n",
        "# Realiza la optimización bayesiana para MLP\n",
        "best_model = bayesian_optimization(MLPModel, train_loader, val_loader, space_mlp, max_evals=50, device=DEVICE)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "k1972LeeeOJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4.1 CNN"
      ],
      "metadata": {
        "id": "T6G7tnlid46C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define el espacio de búsqueda para CNN\n",
        "# space_cnn = {\n",
        "#     'epochs': hp.choice('epochs', epochs_choices),\n",
        "#     'lr': hp.loguniform('lr', np.log(0.001), np.log(0.1)),\n",
        "#     'optimizer': hp.choice('optimizer', optimizer_choices),\n",
        "#     'hidden_dim': hp.choice('hidden_dim', hidden_dim_choices),\n",
        "#     'dropout_rate': hp.uniform('dropout_rate', 0, 0.5)\n",
        "# }\n",
        "\n",
        "# # Realiza la optimización bayesiana para CNN\n",
        "# best_model = bayesian_optimization(CNNModel, train_loader, val_loader, space_cnn, max_evals=50, device=DEVICE)"
      ],
      "metadata": {
        "id": "azWrt01efI-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4.1 RNN"
      ],
      "metadata": {
        "id": "BV65Z64qd6sU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define el espacio de búsqueda para RNN\n",
        "# space_rnn = {\n",
        "#     'epochs': hp.choice('epochs', epochs_choices),\n",
        "#     'lr': hp.loguniform('lr', np.log(0.001), np.log(0.1)),\n",
        "#     'optimizer': hp.choice('optimizer', optimizer_choices),\n",
        "#     'hidden_dim': hp.choice('hidden_dim', hidden_dim_choices),\n",
        "#     'layer_dim': hp.choice('layer_dim', layer_dim_choices),\n",
        "#     'dropout_rate': hp.uniform('dropout_rate', 0, 0.5)\n",
        "# }\n",
        "\n",
        "# # Realiza la optimización bayesiana para RNN\n",
        "# best_model = bayesian_optimization(RNNModel, train_loader, val_loader, space_rnn, max_evals=50, device=DEVICE)"
      ],
      "metadata": {
        "id": "C-aXiElTfMZr",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4.1 LSTM"
      ],
      "metadata": {
        "id": "Ka4QI_0Wd8Qt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define el espacio de búsqueda para LSTM\n",
        "# space_lstm = {\n",
        "#     'epochs': hp.choice('epochs', epochs_choices),\n",
        "#     'lr': hp.loguniform('lr', np.log(0.001), np.log(0.1)),\n",
        "#     'optimizer': hp.choice('optimizer', optimizer_choices),\n",
        "#     'hidden_dim': hp.choice('hidden_dim', hidden_dim_choices),\n",
        "#     'layer_dim': hp.choice('layer_dim', layer_dim_choices),  # Número de capas LSTM\n",
        "#     'dropout_rate': hp.uniform('dropout_rate', 0, 0.5)\n",
        "# }\n",
        "\n",
        "# # Realiza la optimización bayesiana para LSTM\n",
        "# best_model = bayesian_optimization(LSTMModel, train_loader, val_loader, space_lstm, max_evals=50, device=DEVICE)"
      ],
      "metadata": {
        "id": "wgCEykClfPNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5.4.1 GRU"
      ],
      "metadata": {
        "id": "eHT0fwu_d-X1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define el espacio de búsqueda para GRU\n",
        "# space_gru = {\n",
        "#     'epochs': hp.choice('epochs', epochs_choices),\n",
        "#     'lr': hp.loguniform('lr', np.log(0.001), np.log(0.1)),\n",
        "#     'optimizer': hp.choice('optimizer', optimizer_choices),\n",
        "#     'hidden_dim': hp.choice('hidden_dim', hidden_dim_choices),\n",
        "#     'layer_dim': hp.choice('layer_dim', layer_dim_choices),  # Número de capas GRU\n",
        "#     'dropout_rate': hp.uniform('dropout_rate', 0, 0.5)\n",
        "# }\n",
        "\n",
        "# # Realiza la optimización bayesiana para GRU\n",
        "# best_model = bayesian_optimization(GRUModel, train_loader, val_loader, space_gru, max_evals=50, device=DEVICE)"
      ],
      "metadata": {
        "id": "yXkRx_iffSDf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mejores hiperpárametros"
      ],
      "metadata": {
        "id": "i_FI_b02AmAM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprime los mejores parámetros y el mejor modelo\n",
        "print(\"Mejores parámetros:\")\n",
        "print(f\"dropout_rate: {best_model['best_params']['dropout_rate']}\")\n",
        "print(f\"epochs: {epochs_choices[best_model['best_params']['epochs']]}\")\n",
        "print(f\"hidden_dim: {hidden_dim_choices[best_model['best_params']['hidden_dim']]}\")\n",
        "print(f\"lr: {best_model['best_params']['lr']}\")\n",
        "print(f\"optimizer: {optimizer_choices[best_model['best_params']['optimizer']]}\")\n",
        "if 'layer_dim' in best_model['best_params']:\n",
        "  print(f\"layer_dim: {layer_dim_choices[best_model['best_params']['layer_dim']]}\")\n",
        "print(\"Mejor modelo:\", best_model['best_model'])"
      ],
      "metadata": {
        "id": "BfbFkN59Mks0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Predicción en el Test Set"
      ],
      "metadata": {
        "id": "LVdjsQW78pIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.1 Crear Secuencias para Test\n",
        "Combinamos train y test para obtener las semanas previas necesarias."
      ],
      "metadata": {
        "id": "etAZbHts8uo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([train_df.drop(columns=[target]), test_df], sort=False)\n",
        "combined_df = combined_df.sort_values(by=['id_bar', 'fecha'])\n",
        "\n",
        "test_sequences = []\n",
        "ids = []\n",
        "\n",
        "for idx, row in test_df.iterrows():\n",
        "    id_bar = row['id_bar']\n",
        "    fecha = row.name\n",
        "    prev_dates = combined_df[(combined_df['id_bar'] == id_bar) & (combined_df.index < fecha)].tail(config[\"WINDOW_SIZE\"])\n",
        "    if len(prev_dates) == config[\"WINDOW_SIZE\"]:\n",
        "        seq = prev_dates[features].values\n",
        "        test_sequences.append(seq)\n",
        "        ids.append(row['id'])\n",
        "\n",
        "test_sequences = np.array(test_sequences)\n",
        "test_tensor = torch.tensor(test_sequences, dtype=torch.float32).to(DEVICE)"
      ],
      "metadata": {
        "id": "ynhDKcV98ys6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##6.2 Entrenar Modelo Final y Predecir"
      ],
      "metadata": {
        "id": "sgoAdOEI83Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_trained = best_model['best_model'].to(DEVICE)"
      ],
      "metadata": {
        "id": "YDWNB-DX7m9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar dataset de prueba\n",
        "'''\n",
        "test_sequences = []\n",
        "ids = []\n",
        "for id_bar, group_test in test_df.groupby(config[\"GROUP_COLUMN\"]):\n",
        "    group_train = train_df_full[train_df_full[config[\"GROUP_COLUMN\"]] == id_bar]\n",
        "    group_full = pd.concat([group_train, group_test]).sort_index()\n",
        "    for i in range(len(group_test)):\n",
        "        start_idx = len(group_full) - len(group_test) - config[\"WINDOW_SIZE\"] + i\n",
        "        end_idx = start_idx + config[\"WINDOW_SIZE\"]\n",
        "        if start_idx >= 0 and end_idx <= len(group_full) - config[\"HORIZON\"]:\n",
        "            X = group_full.iloc[start_idx:end_idx][features].values\n",
        "            test_sequences.append(X)\n",
        "            ids.append(group_test.iloc[i]['id'])'''\n",
        "\n",
        "test_dataset = DengueTestDataset(test_sequences)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "tDY0iw4X856w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar predicciones\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for X in test_loader:\n",
        "        X = X.to(DEVICE)\n",
        "        output = best_model_trained(X)\n",
        "        predictions.append(scaler_target.inverse_transform(output.cpu().numpy()))\n",
        "predictions = np.concatenate(predictions)\n",
        "predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Preparar submission\n",
        "df_submission = pd.DataFrame({'id': ids, 'dengue': predictions})\n",
        "df_submission.to_csv('submission.csv', index=False)\n",
        "print(f'Submission guardado en submission.csv, con {len(df_submission)} predicciones.')"
      ],
      "metadata": {
        "id": "0-bQaQwJd7b9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}