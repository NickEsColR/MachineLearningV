{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cam2149/MachineLearningV/blob/main/NN-RNN-CNN_vtest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equipo**\n",
        "\n",
        "- Nicolás Colmenares\n",
        "\n",
        "- Carlos Martinez"
      ],
      "metadata": {
        "id": "Ckrr43Eb7-jD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Situación:**\n",
        "Una ciudad enfrenta un aumento significativo de casos de dengue, con una tasa de incidencia que supera el promedio nacional.\n",
        "La anticipación de brotes es crucial para implementar medidas preventivas y reducir la propagación de la enfermedad.\n",
        "\n",
        "**Objetivo:**\n",
        "Desarrollar un modelo predictivo utilizando redes neuronales para pronosticar futuros brotes de dengue en cada barrio de la ciudad.\n",
        "Utilizar una base de datos histórica de casos de dengue desde 2015 hasta 2022 para entrenar el modelo.\n",
        "Anticiparse a los brotes con al menos 3 semanas de anticipación.\n",
        "\n",
        "**Finalidad:**\n",
        "Permitir a las autoridades de salud pública tomar acciones oportunas, como:\n",
        "Preparar a las instituciones prestadoras de salud (IPS).\n",
        "Gestionar recursos (carros fumigadores, limpieza de sumideros).\n",
        "Capacitar a la comunidad."
      ],
      "metadata": {
        "id": "zQ9T88GEx2Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Redes Neuronales Tradicinales (MLP)\n",
        "2. Red Convolucional (CNN) adaptada a series temporales\n",
        "3. Red Neuronal Recurrente (RNN) básica.\n",
        "4. Modelo con LSTMs\n",
        "5. Modelo con GRUs"
      ],
      "metadata": {
        "id": "dVtVgKGCyXfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diccionario"
      ],
      "metadata": {
        "id": "R3PIeauF9c56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.parquet - El conjunto de datos de entrenamiento\n",
        "test.parquet - El conjunto de datos de prueba\n",
        "sample_submission.csv - un ejemplo de un archivo a someter en la competencia"
      ],
      "metadata": {
        "id": "uwGcqMDn-AoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Configuraciones de Colab"
      ],
      "metadata": {
        "id": "l9U_uqnvvYtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mover Kaggle.json a la ubicación correcta después de subirlo"
      ],
      "metadata": {
        "id": "zi1MbjLG81SQ"
      }
    },
    {
      "source": [
        "#Estas líneas son comandos de shell que se ejecutan dentro del Jupyter notebook. Se usan para configurar las credenciales de la API de Kaggle, que son necesarias para descargar conjuntos de datos (datasets) desde Kaggle.\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FkW9-t478uLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/kaggle/output\n",
        "!rm -rf /content/kaggle/input"
      ],
      "metadata": {
        "id": "-YMKmQruoaHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargar dataset de la competencia"
      ],
      "metadata": {
        "id": "VWH9Y2UG9KLD"
      }
    },
    {
      "source": [
        "!kaggle competitions download -c aa-v-2025-i-pronosticos-nn-rnn-cnn"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SnUZl0d-8_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/kaggle/output\n",
        "!mkdir -p /content/kaggle/input"
      ],
      "metadata": {
        "id": "vaLprrm32zGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv aa-v-2025-i-pronosticos-nn-rnn-cnn.zip /content/kaggle/input"
      ],
      "metadata": {
        "id": "s07nq5K1zyuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/kaggle/input/aa-v-2025-i-pronosticos-nn-rnn-cnn.zip -d /content/kaggle/input/"
      ],
      "metadata": {
        "id": "skxqP_oq0VeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/kaggle/input\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "0m2oYgGOxJue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Imports"
      ],
      "metadata": {
        "id": "FiQLr9h-0Cr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime, timedelta # Importing the required modules datetime and timedelta\n"
      ],
      "metadata": {
        "id": "MO0sH9aq9VlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing library versions\n",
        "print('Pandas:', pd.__version__)\n",
        "print('Numpy:', np.__version__)\n",
        "print('PyTorch:', torch.__version__)"
      ],
      "metadata": {
        "id": "0AXL-WbwzzVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "AP-TlyoQ0PMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Configuración Inicial y Carga de Datos"
      ],
      "metadata": {
        "id": "ul8b3MgvAEJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"TRAIN_DIR\": '/content/kaggle/input/df_train.parquet',\n",
        "    \"TEST_DIR\": '/content/kaggle/input/df_test.parquet',\n",
        "    \"SUBMISSION_DIR\": '/content/sample_submission.csv',\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"TARGET_COLUMN\": 'dengue',\n",
        "    \"GROUP_COLUMN\": 'id_bar',\n",
        "    \"WINDOW_SIZE\": 5,\n",
        "    \"HORIZON\": 3,\n",
        "}"
      ],
      "metadata": {
        "id": "tnCzjmsJAGst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración del dispositivo\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "-BCn_ktm0cF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datos\n",
        "train_df = pd.read_parquet(config[\"TRAIN_DIR\"])\n",
        "test_df = pd.read_parquet(config[\"TEST_DIR\"])"
      ],
      "metadata": {
        "id": "R4lWtxiS1gNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Preprocesamiento de Datos"
      ],
      "metadata": {
        "id": "eNy7oatl37rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 Generar Columna fecha\n",
        "Creamos la columna fecha basada en anio y semana, asignando el último día de cada semana como índice."
      ],
      "metadata": {
        "id": "HYmewRQw4EwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar columna 'fecha' (último día de la semana, domingo)\n",
        "def last_day_of_week(year, week):\n",
        "    first_day = datetime.strptime(f'{year} {week} 1', \"%Y %W %w\")\n",
        "    days_ahead = 6 - first_day.weekday()\n",
        "    last_day = first_day + timedelta(days=days_ahead)\n",
        "    return last_day\n",
        "\n",
        "train_df['fecha'] = train_df.apply(lambda row: last_day_of_week(row['anio'], row['semana']), axis=1)\n",
        "test_df['fecha'] = test_df.apply(lambda row: last_day_of_week(row['anio'], row['semana']), axis=1)"
      ],
      "metadata": {
        "id": "DMjHYBgM37EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Establecer 'fecha' como índice\n",
        "train_df.set_index('fecha', inplace=True)\n",
        "test_df.set_index('fecha', inplace=True)"
      ],
      "metadata": {
        "id": "3eyFu1UUZdHh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Eliminar columnas innecesarias\n",
        "#train_df.drop(['id_bar', 'anio', 'semana'], axis=1, inplace=True)\n",
        "#test_df.drop(['id_bar', 'anio', 'semana'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "gLoH_lheZqEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir conjunto de entrenamiento en train y validation\n",
        "train_df_full = train_df.copy()\n",
        "train_df = train_df_full[train_df_full.index.year <= 2020].copy()\n",
        "val_df = train_df_full[train_df_full.index.year >= 2021].copy()"
      ],
      "metadata": {
        "id": "rtoYCc5_ZtWs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 Selección de Características\n",
        "Definimos las características de entrada, considerando las correlaciones altas entre variables (e.g., lluvia_mean y lluvia_var: 0.82). Para simplificar, usamos todas las características disponibles y dejamos que el modelo aprenda las relaciones."
      ],
      "metadata": {
        "id": "JEsuX1x14WXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir características (excluyendo variables altamente correlacionadas)\n",
        "features = ['ESTRATO', 'area_barrio', 'concentraciones', 'vivienda', 'equipesado', 'sumideros', 'maquina',\n",
        "            'lluvia_mean', 'temperatura_mean', 'temperatura_max']  # Selección basada en correlaciones\n",
        "target = 'dengue'"
      ],
      "metadata": {
        "id": "_0b0rdvB4cBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3 Normalización\n"
      ],
      "metadata": {
        "id": "KtgONUzG4fZb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalizamos las características y el objetivo usando StandardScaler. Identificamos las características numéricas (excluyendo id, id_bar y dengue):\n",
        "\n",
        "Características: ESTRATO, area_barrio, concentraciones, vivienda, equipesado, sumideros, maquina, lluvia_mean, lluvia_var, lluvia_max, lluvia_min, temperatura_mean, temperatura_var, temperatura_max, temperatura_min. Ajustamos escaladores por separado para características y objetivo:"
      ],
      "metadata": {
        "id": "3kTHM_xFSOuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se excluyeron variables como lluvia_var, lluvia_max, temperatura_var, y temperatura_min debido a sus altas correlaciones (e.g., lluvia_var y lluvia_mean: 0.82), para reducir redundancia y mejorar la estabilidad de los modelos."
      ],
      "metadata": {
        "id": "oJpWlJ8kZ9qe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalización\n",
        "scaler_features = StandardScaler()\n",
        "train_df[features] = scaler_features.fit_transform(train_df[features])\n",
        "val_df[features] = scaler_features.transform(val_df[features])\n",
        "test_df[features] = scaler_features.transform(test_df[features])\n",
        "\n",
        "scaler_target = StandardScaler()\n",
        "train_df[target] = scaler_target.fit_transform(train_df[[target]])\n",
        "val_df[target] = scaler_target.transform(val_df[[target]])"
      ],
      "metadata": {
        "id": "g9SU3JzO4jua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3 Crear Secuencias para Series Temporales\n",
        "Para predecir con 3 semanas de anticipación, usamos una ventana de 5 semanas (window_size=5) y un horizonte de 3 semanas (horizon=3)."
      ],
      "metadata": {
        "id": "uea5HBGF5y9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(df, window_size, horizon, features, target, group_column):\n",
        "    sequences = []\n",
        "    labels = []\n",
        "    groups = df.groupby(group_column)\n",
        "    for _, group in groups:\n",
        "        group = group.sort_index()\n",
        "        for i in range(len(group) - window_size - horizon + 1):\n",
        "            X = group.iloc[i:i + window_size][features].values\n",
        "            y = group.iloc[i + window_size + horizon - 1][target]\n",
        "            sequences.append(X)\n",
        "            labels.append(y)\n",
        "    return sequences, labels\n",
        "\n",
        "train_sequences, train_labels = create_sequences(train_df, config[\"WINDOW_SIZE\"], config[\"HORIZON\"], features, target, config[\"GROUP_COLUMN\"])\n",
        "val_sequences, val_labels = create_sequences(val_df, config[\"WINDOW_SIZE\"], config[\"HORIZON\"], features, target, config[\"GROUP_COLUMN\"])"
      ],
      "metadata": {
        "id": "EuEmBfZ-56bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.4 Dataset y DataLoader\n",
        "Creamos un Dataset personalizado y dividimos en entrenamiento y validación."
      ],
      "metadata": {
        "id": "QoVFGjmM5_bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DengueDataset(Dataset):\n",
        "    def __init__(self, sequences, labels):\n",
        "        self.sequences = sequences\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.sequences[idx]\n",
        "        y = self.labels[idx]\n",
        "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "EM4JWY_o6EYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DengueTestDataset(Dataset):\n",
        "    def __init__(self, sequences):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.sequences[idx]\n",
        "        return torch.tensor(X, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "EPVVoaE_diEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = DengueDataset(train_sequences, train_labels)\n",
        "val_dataset = DengueDataset(val_sequences, val_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=config[\"BATCH_SIZE\"], shuffle=False)"
      ],
      "metadata": {
        "id": "6_RtD605chV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Implementación de Modelos"
      ],
      "metadata": {
        "id": "VbrI6Wbo6ljL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.1 Modelo MLP\n",
        "Un Perceptrón Multicapa que aplana las secuencias."
      ],
      "metadata": {
        "id": "dhYPwPA66pMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.2):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)\n",
        "        return self.model(x)"
      ],
      "metadata": {
        "id": "KXQl0Ajm6sIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.2 Modelo CNN para Series Temporales\n",
        "Una CNN 1D adaptada a series temporales."
      ],
      "metadata": {
        "id": "YfsmgmEq7H1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.2):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, window_size)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x).squeeze(-1) # La salida es (batch_size, hidden_dim)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "r0LS148fv7q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.3 Modelo RNN Básico\n",
        "Implementación proporcionada con estados iniciales definidos."
      ],
      "metadata": {
        "id": "RXjpZx3m7M0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_rate=0.2):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.rnn(x, h0)\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "uXl5Db3cxCnT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.4 Modelo LSTM"
      ],
      "metadata": {
        "id": "91i05YyF7RhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_rate=0.2):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.lstm(x, (h0, c0))\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "1mipmz3M7V7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.5 Modelo GRU"
      ],
      "metadata": {
        "id": "EAtxxjkp7awI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim, dropout_rate=0.2):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(x.device)\n",
        "        out, _ = self.gru(x, h0)\n",
        "        out = self.dropout(out[:, -1, :])\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "id": "zIYK3qdt7c6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Entrenamiento y Evaluación"
      ],
      "metadata": {
        "id": "uTEzHGf97gyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.1 Función de Entrenamiento"
      ],
      "metadata": {
        "id": "__VP-McM7jYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs, optimizer, criterion, device):\n",
        "    model.to(device)\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for X, y in val_loader:\n",
        "                X, y = X.to(device), y.to(device)\n",
        "                output = model(X)\n",
        "                loss = criterion(output, y)\n",
        "                val_loss += loss.item()\n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:  # Imprimir solo cada 10 épocas\n",
        "          print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "5u1w28uH7lcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.2 Función de Evaluación"
      ],
      "metadata": {
        "id": "PxuM2Xd47vji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, val_loader, device, scaler_target):\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    actuals = []\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            output = model(X)\n",
        "            predictions.append(output.cpu().numpy())\n",
        "            actuals.append(y.cpu().numpy())\n",
        "    predictions = np.concatenate(predictions)\n",
        "    actuals = np.concatenate(actuals)\n",
        "    predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "    actuals = scaler_target.inverse_transform(actuals.reshape(-1, 1)).flatten()\n",
        "    mae = mean_absolute_error(actuals, predictions)\n",
        "    mse = mean_squared_error(actuals, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
        "    return mae, mse, rmse, predictions, actuals"
      ],
      "metadata": {
        "id": "NacHAVl_71M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.3 Gráficos\n",
        "Generamos gráficos de pérdidas y predicciones vs reales."
      ],
      "metadata": {
        "id": "UpDtMX208FxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses, val_losses):\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.title('Training and Validation Losses')\n",
        "    plt.show()\n",
        "\n",
        "def plot_predictions(actuals, predictions):\n",
        "    plt.plot(actuals, label='Actual')\n",
        "    plt.plot(predictions, label='Predicted')\n",
        "    plt.xlabel('Sample')\n",
        "    plt.ylabel('Dengue Cases')\n",
        "    plt.legend()\n",
        "    plt.title('Actual vs Predicted Values')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "kDnTvwsS8FMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.4 Entrenar Modelos\n",
        "Entrenamos cada modelo con hiperparámetros fijos para comparación inicial."
      ],
      "metadata": {
        "id": "TcSszRDU749v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "models = {\n",
        "    'MLP': MLPModel(input_dim=config[\"WINDOW_SIZE\"] * len(features), hidden_dim=64, output_dim=1),\n",
        "    'CNN': CNNModel(input_dim=len(features), hidden_dim=32, output_dim=1),\n",
        "    'RNN': RNNModel(input_dim=len(features), hidden_dim=64, layer_dim=1, output_dim=1),\n",
        "    'LSTM': LSTMModel(input_dim=len(features), hidden_dim=64, layer_dim=1, output_dim=1),\n",
        "    'GRU': GRUModel(input_dim=len(features), hidden_dim=64, layer_dim=1, output_dim=1)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "criterion = nn.MSELoss()\n",
        "device = DEVICE\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f'\\nTraining {name}...')\n",
        "    optimizer = optim.RMSprop(model.parameters(), lr=0.001)\n",
        "    train_losses, val_losses = train_model(model, train_loader, val_loader, epochs=100, optimizer=optimizer, criterion=criterion, device=device)\n",
        "    mae, mse, rmse, preds, actuals = evaluate_model(model, val_loader, device, scaler_target)\n",
        "    results[name] = {'MAE': mae, 'MSE': mse, 'RMSE': rmse, 'train_losses': train_losses, 'val_losses': val_losses}\n",
        "    plot_losses(train_losses, val_losses)\n",
        "    plot_predictions(actuals, preds)\n",
        "    torch.save(model.state_dict(), f'{name}_model.pth')  # Guardar modelo"
      ],
      "metadata": {
        "id": "4WPypA4979SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Selección del Mejor Modelo\n",
        "Realizamos una búsqueda simple sobre hiperparámetros para cada modelo y seleccionamos el mejor basado en RMSE."
      ],
      "metadata": {
        "id": "G9GXaihI8Nh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "best_model_name = min(results, key=lambda x: results[x]['RMSE'])\n",
        "print(f'\\nMejor modelo: {best_model_name} con RMSE: {results[best_model_name][\"RMSE\"]:.4f}')"
      ],
      "metadata": {
        "id": "0eRBn-qW8MZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Predicción en el Test Set con MLP\n",
        "Evaluamos el modelo MLP en el conjunto de test."
      ],
      "metadata": {
        "id": "LVdjsQW78pIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.1 Crear Secuencias para Test\n",
        "Combinamos train y test para obtener las semanas previas necesarias."
      ],
      "metadata": {
        "id": "etAZbHts8uo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([train_df.drop(columns=[target]), test_df], sort=False)\n",
        "combined_df = combined_df.sort_values(by=['id_bar', 'fecha'])\n",
        "\n",
        "test_sequences = []\n",
        "ids = []\n",
        "\n",
        "for idx, row in test_df.iterrows():\n",
        "    id_bar = row['id_bar']\n",
        "    fecha = row.name\n",
        "    prev_dates = combined_df[(combined_df['id_bar'] == id_bar) & (combined_df.index < fecha)].tail(config[\"WINDOW_SIZE\"])\n",
        "    if len(prev_dates) == config[\"WINDOW_SIZE\"]:\n",
        "        seq = prev_dates[features].values\n",
        "        test_sequences.append(seq)\n",
        "        ids.append(row['id'])\n",
        "\n",
        "test_sequences = np.array(test_sequences)\n",
        "test_tensor = torch.tensor(test_sequences, dtype=torch.float32).to(DEVICE)"
      ],
      "metadata": {
        "id": "ynhDKcV98ys6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.2 Entrenar MLP Final y Predecir"
      ],
      "metadata": {
        "id": "sgoAdOEI83Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preparar dataset de prueba\n",
        "'''\n",
        "test_sequences = []\n",
        "ids = []\n",
        "for id_bar, group_test in test_df.groupby(config[\"GROUP_COLUMN\"]):\n",
        "    group_train = train_df_full[train_df_full[config[\"GROUP_COLUMN\"]] == id_bar]\n",
        "    group_full = pd.concat([group_train, group_test]).sort_index()\n",
        "    for i in range(len(group_test)):\n",
        "        start_idx = len(group_full) - len(group_test) - config[\"WINDOW_SIZE\"] + i\n",
        "        end_idx = start_idx + config[\"WINDOW_SIZE\"]\n",
        "        if start_idx >= 0 and end_idx <= len(group_full) - config[\"HORIZON\"]:\n",
        "            X = group_full.iloc[start_idx:end_idx][features].values\n",
        "            test_sequences.append(X)\n",
        "            ids.append(group_test.iloc[i]['id'])'''\n",
        "\n",
        "test_dataset = DengueTestDataset(test_sequences)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "tDY0iw4X856w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar modelo MLP entrenado\n",
        "mlp_model = MLPModel(input_dim=config[\"WINDOW_SIZE\"] * len(features), hidden_dim=64, output_dim=1)\n",
        "mlp_model.load_state_dict(torch.load('MLP_model.pth'))\n",
        "mlp_model.to(device)\n",
        "mlp_model.eval()"
      ],
      "metadata": {
        "id": "awvHdhbad3uy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar predicciones\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for X in test_loader:\n",
        "        X = X.to(device)\n",
        "        output = mlp_model(X)\n",
        "        predictions.append(output.cpu().numpy())\n",
        "predictions = np.concatenate(predictions)\n",
        "predictions = scaler_target.inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
        "\n",
        "# Preparar submission\n",
        "df_submission = pd.DataFrame({'id': ids, 'dengue': predictions})\n",
        "df_submission.to_csv('submission.csv', index=False)\n",
        "print(f'Submission guardado en submission.csv, con {len(df_submission)} predicciones.')"
      ],
      "metadata": {
        "id": "0-bQaQwJd7b9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "\n",
        "class DengueTestDataset(Dataset):\n",
        "    def __init__(self, sequences, ids): # Added ids to __init__\n",
        "        self.sequences = sequences\n",
        "        self.ids = ids # Store ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        X = self.sequences[idx]\n",
        "        batch_id = self.ids[idx] # Get the corresponding id\n",
        "        return torch.tensor(X, dtype=torch.float32), batch_id # Return both sequence and id\n",
        "\n",
        "# Función para entrenar y evaluar\n",
        "def train_and_evaluate(model, train_dataset, val_dataset, epochs, learning_rate, optimizer_class, batch_size, device):\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "    optimizer = optimizer_class(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.MSELoss()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for X, y in train_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in val_loader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            output = model(X)\n",
        "            loss = criterion(output, y)\n",
        "            val_loss += loss.item()\n",
        "    val_loss /= len(val_loader)\n",
        "    return val_loss\n",
        "\n",
        "# Modelos\n",
        "models = {\n",
        "    'MLP': MLPModel,\n",
        "    'CNN': CNNModel,\n",
        "    'RNN': RNNModel,\n",
        "    'LSTM': LSTMModel,\n",
        "    'GRU': GRUModel\n",
        "}\n",
        "\n",
        "# Hiperparámetros\n",
        "#epochs_list = [100, 300, 500]\n",
        "#learning_rates = [0.01, 0.001]\n",
        "#optimizers = [optim.Adam, optim.AdamW, optim.SGD, optim.RMSprop]\n",
        "#batch_sizes = [18, 32, 64]\n",
        "\n",
        "epochs_list = [100]\n",
        "learning_rates = [0.001]\n",
        "optimizers = [optim.RMSprop]\n",
        "batch_sizes = [18, 32, 64]\n",
        "\n",
        "# Dispositivo\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Encontrar el mejor modelo\n",
        "best_val_loss = float('inf')\n",
        "best_model_details = None\n",
        "\n",
        "for model_name, model_class in models.items():\n",
        "    for epochs in epochs_list:\n",
        "        for lr in learning_rates:\n",
        "            for opt_class in optimizers:\n",
        "                for batch_size in batch_sizes:\n",
        "                    print(f'\\nEntrenando {model_name} con epochs={epochs}, lr={lr}, '\n",
        "                          f'optimizer={opt_class.__name__}, batch_size={batch_size}')\n",
        "                    if model_name == 'MLP':\n",
        "                        model = model_class(input_dim=config[\"WINDOW_SIZE\"] * len(features),\n",
        "                                          hidden_dim=64, output_dim=1)\n",
        "                    elif model_name == 'CNN':\n",
        "                        model = model_class(input_channels=len(features),\n",
        "                                          hidden_dim=32, output_dim=1)\n",
        "                    else:\n",
        "                        model = model_class(input_dim=len(features),\n",
        "                                          hidden_dim=64, layer_dim=1, output_dim=1)\n",
        "                    val_loss = train_and_evaluate(model, train_dataset, val_dataset,\n",
        "                                                epochs, lr, opt_class, batch_size, device)\n",
        "                    if val_loss < best_val_loss:\n",
        "                        best_val_loss = val_loss\n",
        "                        best_model_details = {\n",
        "                            'model_name': model_name,\n",
        "                            'hyperparams': {\n",
        "                                'epochs': epochs,\n",
        "                                'lr': lr,\n",
        "                                'optimizer': opt_class.__name__,\n",
        "                                'batch_size': batch_size\n",
        "                            },\n",
        "                            'state_dict': copy.deepcopy(model.state_dict())\n",
        "                        }\n",
        "\n",
        "print(f'\\nMejor modelo: {best_model_details[\"model_name\"]} con val_loss: {best_val_loss:.4f} '\n",
        "      f'y hiperparámetros: {best_model_details[\"hyperparams\"]}')\n",
        "\n",
        "# Cargar el mejor modelo\n",
        "best_model_name = best_model_details['model_name']\n",
        "if best_model_name == 'MLP':\n",
        "    best_model = MLPModel(input_dim=config[\"WINDOW_SIZE\"] * len(features),\n",
        "                         hidden_dim=64, output_dim=1)\n",
        "elif best_model_name == 'CNN':\n",
        "    best_model = CNNModel(input_channels=len(features),\n",
        "                         hidden_dim=32, output_dim=1)\n",
        "else:\n",
        "    best_model = models[best_model_name](input_dim=len(features),\n",
        "                                       hidden_dim=64, layer_dim=1, output_dim=1)\n",
        "best_model.load_state_dict(best_model_details['state_dict'])\n",
        "best_model.to(device)\n",
        "best_model.eval()\n"
      ],
      "metadata": {
        "id": "YRN7cgDUvBCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Zi1admywH1dC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mejor modelo: LSTM con val_loss: 0.3058 y hiperparámetros: {'epochs': 100, 'lr': 0.001, 'optimizer': 'AdamW', 'batch_size': 32}\n",
        "LSTMModel(\n",
        "  (lstm): LSTM(10, 64, batch_first=True)\n",
        "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
        ")"
      ],
      "metadata": {
        "id": "Ss1DDHAWH0kV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mejor modelo: GRU con val_loss: 0.3039 y hiperparámetros: {'epochs': 100, 'lr': 0.001, 'optimizer': 'SGD', 'batch_size': 32}\n",
        "GRUModel(\n",
        "  (gru): GRU(10, 64, batch_first=True)\n",
        "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
        ")"
      ],
      "metadata": {
        "id": "ixTlua6DORsK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mejor modelo: GRU con val_loss: 0.2979 y hiperparámetros: {'epochs': 100, 'lr': 0.001, 'optimizer': 'RMSprop', 'batch_size': 32}\n",
        "GRUModel(\n",
        "  (gru): GRU(10, 64, batch_first=True)\n",
        "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
        ")"
      ],
      "metadata": {
        "id": "YthwyBPJrtjS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "l_wdVOHb0GcE"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}