{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cam2149/MachineLearningV/blob/main/CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equipo**\n",
        "\n",
        "- Nicolás Colmenares\n",
        "\n",
        "- Carlos Martinez"
      ],
      "metadata": {
        "id": "Ckrr43Eb7-jD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Implementación de una Red Convolucional (CNN) adaptada a series temporales.- 1 pts\n",
        "\n",
        "  -  .\n",
        "\n",
        "  - .\n",
        "\n",
        "  - ."
      ],
      "metadata": {
        "id": "eNJUawM57uzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Situación:**\n",
        "Una ciudad enfrenta un aumento significativo de casos de dengue, con una tasa de incidencia que supera el promedio nacional.\n",
        "La anticipación de brotes es crucial para implementar medidas preventivas y reducir la propagación de la enfermedad.\n",
        "\n",
        "**Objetivo:**\n",
        "Desarrollar un modelo predictivo utilizando redes neuronales para pronosticar futuros brotes de dengue en cada barrio de la ciudad.\n",
        "Utilizar una base de datos histórica de casos de dengue desde 2015 hasta 2022 para entrenar el modelo.\n",
        "Anticiparse a los brotes con al menos 3 semanas de anticipación.\n",
        "\n",
        "**Finalidad:**\n",
        "Permitir a las autoridades de salud pública tomar acciones oportunas, como:\n",
        "Preparar a las instituciones prestadoras de salud (IPS).\n",
        "Gestionar recursos (carros fumigadores, limpieza de sumideros).\n",
        "Capacitar a la comunidad."
      ],
      "metadata": {
        "id": "zQ9T88GEx2Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Red Convolucional (CNN) adaptada a series temporales.\n",
        "*   .\n",
        "*   ."
      ],
      "metadata": {
        "id": "dVtVgKGCyXfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Configuraciones de Colab"
      ],
      "metadata": {
        "id": "l9U_uqnvvYtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mover Kaggle.json a la ubicación correcta después de subirlo"
      ],
      "metadata": {
        "id": "zi1MbjLG81SQ"
      }
    },
    {
      "source": [
        "#Estas líneas son comandos de shell que se ejecutan dentro del Jupyter notebook. Se usan para configurar las credenciales de la API de Kaggle, que son necesarias para descargar conjuntos de datos (datasets) desde Kaggle.\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FkW9-t478uLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/kaggle/output\n",
        "!rm -rf /content/kaggle/input"
      ],
      "metadata": {
        "id": "-YMKmQruoaHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargar dataset de la competencia"
      ],
      "metadata": {
        "id": "VWH9Y2UG9KLD"
      }
    },
    {
      "source": [
        "!kaggle competitions download -c aa-v-2025-i-pronosticos-nn-rnn-cnn"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SnUZl0d-8_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/kaggle/output\n",
        "!mkdir -p /content/kaggle/input"
      ],
      "metadata": {
        "id": "vaLprrm32zGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv aa-v-2025-i-pronosticos-nn-rnn-cnn.zip /content/kaggle/input"
      ],
      "metadata": {
        "id": "s07nq5K1zyuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/kaggle/input/aa-v-2025-i-pronosticos-nn-rnn-cnn.zip -d /content/kaggle/input/"
      ],
      "metadata": {
        "id": "skxqP_oq0VeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/kaggle/input\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "0m2oYgGOxJue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Imports"
      ],
      "metadata": {
        "id": "FiQLr9h-0Cr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install altair"
      ],
      "metadata": {
        "id": "10HK-Am92SqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "MO0sH9aq9VlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing library versions\n",
        "print('Pandas:', pd.__version__)\n",
        "print('Numpy:', np.__version__)\n",
        "print('PyTorch:', torch.__version__)\n",
        "print('Altair:', alt.__version__)"
      ],
      "metadata": {
        "id": "0AXL-WbwzzVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "AP-TlyoQ0PMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Configs"
      ],
      "metadata": {
        "id": "ul8b3MgvAEJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"TRAIN_DIR\": '/content/kaggle/input/df_train.parquet',\n",
        "    \"TEST_DIR\": '/content/kaggle/input/df_test.parquet',\n",
        "    \"SUBMISSION_DIR\": '/content/sample_submission.csv'\n",
        "}"
      ],
      "metadata": {
        "id": "tnCzjmsJAGst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploración"
      ],
      "metadata": {
        "id": "9VCh0_mP9YFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diccionario"
      ],
      "metadata": {
        "id": "R3PIeauF9c56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.parquet - El conjunto de datos de entrenamiento\n",
        "test.parquet - El conjunto de datos de prueba\n",
        "sample_submission.csv - un ejemplo de un archivo a someter en la competencia"
      ],
      "metadata": {
        "id": "uwGcqMDn-AoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Variable**         | **Descripción**                                                                                      |\n",
        "|-----------------------|------------------------------------------------------------------------------------------------------|\n",
        "| id_bar               | identificador único del barrio                                                                      |\n",
        "| anio                 | Año de ocurrencia                                                                                   |\n",
        "| semana               | Semana de ocurrencia                                                                               |\n",
        "| Estrato              | Estrato socioeconómico del barrio                                                                   |\n",
        "| area_barrio          | Área del barrio en km²                                                                             |\n",
        "| dengue               | Conteo de casos de dengue                                                                          |\n",
        "| concentraciones      | Cantidad de visitas e intervención a lugares de concentración humana (Instituciones)                |\n",
        "| vivienda             | Conteo de las visitas a viviendas a revisión y control de criaderos                                 |\n",
        "| equipesado           | Conteo de las fumigaciones con Maquinaria Pesada                                                   |\n",
        "| sumideros            | Conteo de las intervenciones a los sumideros                                                       |\n",
        "| maquina              | Conteo de las fumigaciones con motomochila                                                         |\n",
        "| lluvia_mean          | Lluvia promedio en la semana i                                                                     |\n",
        "| lluvia_var           | Varianza de la lluvia en la semana i                                                               |\n",
        "| lluvia_max           | Lluvia máxima en la semana i                                                                       |\n",
        "| lluvia_min           | Lluvia mínima en la semana i                                                                       |\n",
        "| temperatura_mean     | Temperatura promedio en la semana i                                                                |\n",
        "| temperatura_var      | Varianza de la temperatura en la semana i                                                          |\n",
        "| temperatura_max      | Temperatura máxima en la semana i                                                                  |\n",
        "| temperatura_min      | Temperatura mínima en la semana i                                                                  |\n"
      ],
      "metadata": {
        "id": "Pw_6kTQ29_UD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lectura del dataset de entrenamiento"
      ],
      "metadata": {
        "id": "eqMzIl1N-EUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Esta celda tiene como objetivo leer los datos de entrenamiento desde un archivo Parquet y mostrar información básica sobre ellos\n",
        "def load_and_prepare_data(train_dir, test_dir):\n",
        "  try:\n",
        "    train_df = pd.read_parquet(train_dir)\n",
        "    test_df = pd.read_parquet(test_dir)\n",
        "\n",
        "    # Generar columna 'id'\n",
        "    for df in [train_df, test_df]:\n",
        "        df['id'] = df['id_bar'].astype(str) + '_' + df['anio'].astype(str) + '_' + df['semana'].astype(str)\n",
        "\n",
        "    # Ordenar por id_bar, anio y semana\n",
        "    train_df = train_df.sort_values(['id_bar', 'anio', 'semana'])\n",
        "    test_df = test_df.sort_values(['id_bar', 'anio', 'semana'])\n",
        "\n",
        "    return train_df, test_df\n",
        "  except FileNotFoundError:\n",
        "    print(\"Error: 'series_train.parquet' not found. Please make sure the file exists in the current directory or provide the correct path.\")\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eJ0n4hef5Dd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_features(train_df, test_df, features):\n",
        "  try:\n",
        "    scaler = StandardScaler()\n",
        "    train_df[features] = scaler.fit_transform(train_df[features])\n",
        "    test_df[features] = scaler.transform(test_df[features])\n",
        "    return train_df, test_df, scaler\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "6iHIV2JAqGQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_sequences(df, window_size, forecast_horizon, features, target):\n",
        "  try:\n",
        "    X, y = [], []\n",
        "    for id_bar in df['id_bar'].unique():\n",
        "        df_bar = df[df['id_bar'] == id_bar].sort_values(['anio', 'semana'])\n",
        "        for i in range(len(df_bar) - window_size - forecast_horizon + 1):\n",
        "            X.append(df_bar.iloc[i:i+window_size][features].values)\n",
        "            y.append(df_bar.iloc[i+window_size+forecast_horizon-1][target])\n",
        "    return np.array(X), np.array(y)\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n",
        "\n",
        "def create_test_sequences(train_df, test_df, window_size, forecast_horizon, features):\n",
        "  try:\n",
        "    X_test = []\n",
        "    train_df['temporal_idx'] = (train_df['anio'] - train_df['anio'].min()) * 52 + train_df['semana']\n",
        "    test_df['temporal_idx'] = (test_df['anio'] - train_df['anio'].min()) * 52 + test_df['semana']\n",
        "\n",
        "    for idx, row in test_df.iterrows():\n",
        "        id_bar = row['id_bar']\n",
        "        t = row['temporal_idx']\n",
        "        start = t - forecast_horizon - window_size\n",
        "        end = t - forecast_horizon - 1\n",
        "        df_bar = train_df[(train_df['id_bar'] == id_bar) &\n",
        "                          (train_df['temporal_idx'] >= start) &\n",
        "                          (train_df['temporal_idx'] <= end)]\n",
        "        if len(df_bar) == window_size:\n",
        "            X_test.append(df_bar[features].values)\n",
        "        else:\n",
        "            # Padding con ceros si no hay suficientes datos\n",
        "            X_test.append(np.zeros((window_size, len(features))))\n",
        "    return np.array(X_test)\n",
        "  except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "kDThOm5DqMko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DengueDataset(Dataset):\n",
        "    def __init__(self, X, y=None):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.float32) if y is not None else None\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.y is not None:\n",
        "            return self.X[idx], self.y[idx]\n",
        "        return self.X[idx]"
      ],
      "metadata": {
        "id": "kFJqBCgh44qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNForecast(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, kernel_size=3):\n",
        "        super(CNNForecast, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(input_size, hidden_size, kernel_size)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        # Calcular tamaño de salida después de conv y pool\n",
        "        conv_output_size = (window_size - kernel_size + 1) // 2\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(hidden_size * conv_output_size, 50)\n",
        "        self.fc2 = nn.Linear(50, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)  # (batch, features, sequence)\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.flatten(x)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EvuqJzF0a3ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device):\n",
        "    model.to(device)\n",
        "    best_mse = float('inf')\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in tqdm(range(epochs), desc=\"Training\"):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred.squeeze(), y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "\n",
        "        # Evaluación en validación\n",
        "        model.eval()\n",
        "        val_preds, val_true = [], []\n",
        "        with torch.no_grad():\n",
        "            for X_val, y_val in val_loader:\n",
        "                X_val, y_val = X_val.to(device), y_val.to(device)\n",
        "                y_pred = model(X_val).squeeze()\n",
        "                val_preds.extend(y_pred.cpu().numpy())\n",
        "                val_true.extend(y_val.cpu().numpy())\n",
        "\n",
        "        mse = mean_squared_error(val_true, val_preds)\n",
        "        if mse < best_mse:\n",
        "            best_mse = mse\n",
        "            best_model_state = model.state_dict()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch}, Loss: {running_loss/len(train_loader):.4f}, Val MSE: {mse:.4f}\")\n",
        "\n",
        "    model.load_state_dict(best_model_state)\n",
        "    return model, compute_metrics(val_true, val_preds)\n",
        "\n",
        "def compute_metrics(y_true, y_pred):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    # Accuracy como proporción de predicciones cercanas (umbral arbitrario de 0.5)\n",
        "    y_true_bin = [1 if x > 0.5 else 0 for x in y_true]\n",
        "    y_pred_bin = [1 if x > 0.5 else 0 for x in y_pred]\n",
        "    accuracy = accuracy_score(y_true_bin, y_pred_bin)\n",
        "    return {\"MAE\": mae, \"MSE\": mse, \"RMSE\": rmse, \"Accuracy\": accuracy}"
      ],
      "metadata": {
        "id": "oVYF9iqSEv-0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_best_model(X_train, y_train, input_size, hidden_size=64, output_size=1, device='cpu'):\n",
        "    epochs_list = [100, 300, 500]\n",
        "    learning_rates = [0.01, 0.001]\n",
        "    optimizers = [optim.Adam, optim.SGD]\n",
        "\n",
        "    # Dividir en entrenamiento y validación\n",
        "    split_idx = int(0.8 * len(X_train))\n",
        "    train_dataset = DengueDataset(X_train[:split_idx], y_train[:split_idx])\n",
        "    val_dataset = DengueDataset(X_train[split_idx:], y_train[split_idx:])\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=32)\n",
        "\n",
        "    best_metrics = None\n",
        "    best_config = None\n",
        "    best_model = None\n",
        "\n",
        "    for epochs in epochs_list:\n",
        "        for lr in learning_rates:\n",
        "            for opt_class in optimizers:\n",
        "                model = CNNForecast(input_size, hidden_size, output_size)\n",
        "                optimizer = opt_class(model.parameters(), lr=lr)\n",
        "                criterion = nn.MSELoss()\n",
        "\n",
        "                print(f\"\\nTraining with epochs={epochs}, lr={lr}, optimizer={opt_class.__name__}\")\n",
        "                model, metrics = train_model(model, train_loader, val_loader, criterion, optimizer, epochs, device)\n",
        "\n",
        "                if best_metrics is None or metrics['MSE'] < best_metrics['MSE']:\n",
        "                    best_metrics = metrics\n",
        "                    best_config = {'epochs': epochs, 'lr': lr, 'optimizer': opt_class.__name__}\n",
        "                    best_model = model\n",
        "\n",
        "    return best_model, best_metrics, best_config"
      ],
      "metadata": {
        "id": "xbOwGEvfEx-I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_and_submit(model, X_test, test_df, submission_dir, device):\n",
        "    model.eval()\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
        "    with torch.no_grad():\n",
        "        y_pred = model(X_test_tensor).squeeze().cpu().numpy()\n",
        "\n",
        "    submission_df = test_df[['id']].copy()\n",
        "    submission_df['dengue'] = y_pred\n",
        "    submission_df.to_csv(submission_dir, index=False)\n",
        "    return submission_df"
      ],
      "metadata": {
        "id": "1FIo1ldGE06u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "window_size = 10\n",
        "forecast_horizon = 3\n",
        "features = ['ESTRATO', 'area_barrio', 'concentraciones', 'vivienda', 'equipesado', 'sumideros',\n",
        "            'maquina', 'lluvia_mean', 'lluvia_var', 'lluvia_max', 'lluvia_min',\n",
        "            'temperatura_mean', 'temperatura_var', 'temperatura_max', 'temperatura_min']\n",
        "\n",
        "# Cargar y preparar datos\n",
        "train_df, test_df = load_and_prepare_data(config[\"TRAIN_DIR\"], config[\"TEST_DIR\"])\n",
        "train_df, test_df, scaler = normalize_features(train_df, test_df, features)\n",
        "\n",
        "# Crear secuencias\n",
        "X_train, y_train = create_sequences(train_df, window_size, forecast_horizon, features, 'dengue')\n",
        "X_test = create_test_sequences(train_df, test_df, window_size, forecast_horizon, features)\n",
        "\n",
        "# Encontrar el mejor modelo\n",
        "input_size = len(features)\n",
        "best_model, best_metrics, best_config = find_best_model(X_train, y_train, input_size, device=device)\n",
        "\n",
        "# Imprimir resumen\n",
        "print(\"\\nResumen del Mejor Modelo:\")\n",
        "print(f\"Configuración: {best_config}\")\n",
        "print(f\"Métricas: {best_metrics}\")\n",
        "\n",
        "# Predicción y submission\n",
        "submission_df = predict_and_submit(best_model, X_test, test_df, config[\"SUBMISSION_DIR\"], device)\n",
        "\n",
        "print(\"\\nPredicciones generadas en:\", config[\"SUBMISSION_DIR\"])"
      ],
      "metadata": {
        "id": "OXjGPMOjE2Vj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}