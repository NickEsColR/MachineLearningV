{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cam2149/MachineLearningV/blob/main/CNNs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equipo**\n",
        "\n",
        "- Nicolás Colmenares\n",
        "\n",
        "- Carlos Martinez"
      ],
      "metadata": {
        "id": "Ckrr43Eb7-jD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Implementación de una Red Convolucional (CNN) adaptada a series temporales.- 1 pts\n",
        "\n",
        "  -  .\n",
        "\n",
        "  - .\n",
        "\n",
        "  - ."
      ],
      "metadata": {
        "id": "eNJUawM57uzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Situación:**\n",
        "Una ciudad enfrenta un aumento significativo de casos de dengue, con una tasa de incidencia que supera el promedio nacional.\n",
        "La anticipación de brotes es crucial para implementar medidas preventivas y reducir la propagación de la enfermedad.\n",
        "\n",
        "**Objetivo:**\n",
        "Desarrollar un modelo predictivo utilizando redes neuronales para pronosticar futuros brotes de dengue en cada barrio de la ciudad.\n",
        "Utilizar una base de datos histórica de casos de dengue desde 2015 hasta 2022 para entrenar el modelo.\n",
        "Anticiparse a los brotes con al menos 3 semanas de anticipación.\n",
        "\n",
        "**Finalidad:**\n",
        "Permitir a las autoridades de salud pública tomar acciones oportunas, como:\n",
        "Preparar a las instituciones prestadoras de salud (IPS).\n",
        "Gestionar recursos (carros fumigadores, limpieza de sumideros).\n",
        "Capacitar a la comunidad."
      ],
      "metadata": {
        "id": "zQ9T88GEx2Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Red Convolucional (CNN) adaptada a series temporales.\n",
        "*   .\n",
        "*   ."
      ],
      "metadata": {
        "id": "dVtVgKGCyXfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Configuraciones de Colab"
      ],
      "metadata": {
        "id": "l9U_uqnvvYtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mover Kaggle.json a la ubicación correcta después de subirlo"
      ],
      "metadata": {
        "id": "zi1MbjLG81SQ"
      }
    },
    {
      "source": [
        "#Estas líneas son comandos de shell que se ejecutan dentro del Jupyter notebook. Se usan para configurar las credenciales de la API de Kaggle, que son necesarias para descargar conjuntos de datos (datasets) desde Kaggle.\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FkW9-t478uLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/kaggle/output\n",
        "!rm -rf /content/kaggle/input"
      ],
      "metadata": {
        "id": "-YMKmQruoaHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargar dataset de la competencia"
      ],
      "metadata": {
        "id": "VWH9Y2UG9KLD"
      }
    },
    {
      "source": [
        "!kaggle competitions download -c aa-v-2025-i-pronosticos-nn-rnn-cnn"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SnUZl0d-8_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/kaggle/output\n",
        "!mkdir -p /content/kaggle/input"
      ],
      "metadata": {
        "id": "vaLprrm32zGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv aa-v-2025-i-pronosticos-nn-rnn-cnn.zip /content/kaggle/input"
      ],
      "metadata": {
        "id": "s07nq5K1zyuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/kaggle/input/aa-v-2025-i-pronosticos-nn-rnn-cnn.zip -d /content/kaggle/input/"
      ],
      "metadata": {
        "id": "skxqP_oq0VeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/kaggle/input\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "0m2oYgGOxJue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Imports"
      ],
      "metadata": {
        "id": "FiQLr9h-0Cr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install altair"
      ],
      "metadata": {
        "id": "10HK-Am92SqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "import datetime\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import altair as alt\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt   # data visualization\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import scipy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, recall_score, roc_auc_score, accuracy_score, roc_curve\n",
        "from datetime import datetime, timedelta\n",
        "from tqdm import tqdm\n"
      ],
      "metadata": {
        "id": "MO0sH9aq9VlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing library versions\n",
        "print('Pandas:', pd.__version__)\n",
        "print('Statsmodels:', sm.__version__)\n",
        "print('Scipy:', scipy.__version__)\n",
        "print('Matplotlib:', mpl.__version__)\n",
        "print('Seaborn:', sns.__version__)"
      ],
      "metadata": {
        "id": "0AXL-WbwzzVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "AP-TlyoQ0PMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Configs"
      ],
      "metadata": {
        "id": "ul8b3MgvAEJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"TRAIN_DIR\": '/content/kaggle/input/df_train.parquet',\n",
        "    \"TEST_DIR\": '/content/kaggle/input/df_test.parquet',\n",
        "    \"SUBMISSION_DIR\": '/content/sample_submission.csv'\n",
        "}"
      ],
      "metadata": {
        "id": "tnCzjmsJAGst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploración"
      ],
      "metadata": {
        "id": "9VCh0_mP9YFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diccionario"
      ],
      "metadata": {
        "id": "R3PIeauF9c56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.parquet - El conjunto de datos de entrenamiento\n",
        "test.parquet - El conjunto de datos de prueba\n",
        "sample_submission.csv - un ejemplo de un archivo a someter en la competencia"
      ],
      "metadata": {
        "id": "uwGcqMDn-AoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Variable**         | **Descripción**                                                                                      |\n",
        "|-----------------------|------------------------------------------------------------------------------------------------------|\n",
        "| id_bar               | identificador único del barrio                                                                      |\n",
        "| anio                 | Año de ocurrencia                                                                                   |\n",
        "| semana               | Semana de ocurrencia                                                                               |\n",
        "| Estrato              | Estrato socioeconómico del barrio                                                                   |\n",
        "| area_barrio          | Área del barrio en km²                                                                             |\n",
        "| dengue               | Conteo de casos de dengue                                                                          |\n",
        "| concentraciones      | Cantidad de visitas e intervención a lugares de concentración humana (Instituciones)                |\n",
        "| vivienda             | Conteo de las visitas a viviendas a revisión y control de criaderos                                 |\n",
        "| equipesado           | Conteo de las fumigaciones con Maquinaria Pesada                                                   |\n",
        "| sumideros            | Conteo de las intervenciones a los sumideros                                                       |\n",
        "| maquina              | Conteo de las fumigaciones con motomochila                                                         |\n",
        "| lluvia_mean          | Lluvia promedio en la semana i                                                                     |\n",
        "| lluvia_var           | Varianza de la lluvia en la semana i                                                               |\n",
        "| lluvia_max           | Lluvia máxima en la semana i                                                                       |\n",
        "| lluvia_min           | Lluvia mínima en la semana i                                                                       |\n",
        "| temperatura_mean     | Temperatura promedio en la semana i                                                                |\n",
        "| temperatura_var      | Varianza de la temperatura en la semana i                                                          |\n",
        "| temperatura_max      | Temperatura máxima en la semana i                                                                  |\n",
        "| temperatura_min      | Temperatura mínima en la semana i                                                                  |\n"
      ],
      "metadata": {
        "id": "Pw_6kTQ29_UD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lectura del dataset de entrenamiento"
      ],
      "metadata": {
        "id": "eqMzIl1N-EUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Esta celda tiene como objetivo leer los datos de entrenamiento desde un archivo Parquet y mostrar información básica sobre ellos\n",
        "try:\n",
        "    train_df = pd.read_parquet(config[\"TRAIN_DIR\"]).iloc[:,1:]\n",
        "    test_df = pd.read_parquet(config[\"TEST_DIR\"]).iloc[:,1:]\n",
        "    print(train_df.describe())\n",
        "    print(train_df.info())\n",
        "    print(train_df.shape)\n",
        "\n",
        "    print(test_df.describe())\n",
        "    print(test_df.info())\n",
        "    print(test_df.shape)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'series_train.parquet' not found. Please make sure the file exists in the current directory or provide the correct path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "eJ0n4hef5Dd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar la columna 'fecha'\n",
        "def get_last_day_of_week(year, week):\n",
        "  first_day_of_year = datetime(int(year), 1, 1)\n",
        "  return first_day_of_year + timedelta(days=(int(week)-1) * 7)"
      ],
      "metadata": {
        "id": "6iHIV2JAqGQu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " # Aplicar la función para generar la columna de fecha\n",
        "try:\n",
        "  train_df['fecha'] = train_df.apply(lambda row: get_last_day_of_week(int(row['anio']), int(row['semana'])), axis=1)\n",
        "  test_df['fecha'] = test_df.apply(lambda row: get_last_day_of_week(int(row['anio']), int(row['semana'])), axis=1)\n",
        "  # Establecer fecha como índice\n",
        "  train_df = train_df.set_index('fecha')\n",
        "  test_df = test_df.set_index('fecha')\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")"
      ],
      "metadata": {
        "id": "kDThOm5DqMko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para preparar las secuencias de datos para el modelo CNN\n",
        "def preparar_sequences(df, sequence_length=12, forecast_horizon=3, target_col='dengue'):\n",
        "\n",
        "    # Definir las columnas de características (todas excepto la variable objetivo)\n",
        "    feature_cols = [col for col in df.columns if col not in ['id_bar', 'anio', 'semana', target_col]]\n",
        "\n",
        "    # Normalizar las características\n",
        "    scaler = StandardScaler()\n",
        "    df_scaled = pd.DataFrame(scaler.fit_transform(df[feature_cols]),\n",
        "                             columns=feature_cols,\n",
        "                             index=df.index)\n",
        "\n",
        "    # Agregar la variable objetivo\n",
        "    if target_col in df.columns:\n",
        "        df_scaled[target_col] = df[target_col]\n",
        "\n",
        "    # Crear secuencias\n",
        "    X, y = [], []\n",
        "\n",
        "    # Agrupar por barrio (id_bar)\n",
        "    for barrio_id in df['id_bar'].unique():\n",
        "        # Filtrar datos por barrio\n",
        "        barrio_data = df[df['id_bar'] == barrio_id]\n",
        "\n",
        "        # Crear secuencias para este barrio\n",
        "        for i in range(len(barrio_data) - sequence_length - forecast_horizon + 1):\n",
        "            # Secuencia de entrada\n",
        "            sequence = df_scaled.loc[barrio_data.index[i:i+sequence_length], feature_cols].values\n",
        "\n",
        "            # Valor objetivo (casos de dengue después del horizonte de pronóstico)\n",
        "            if target_col in df.columns:\n",
        "                target = barrio_data.iloc[i+sequence_length+forecast_horizon-1][target_col]\n",
        "\n",
        "                X.append(sequence)\n",
        "                y.append(target)\n",
        "\n",
        "    if target_col in df.columns:\n",
        "        return np.array(X), np.array(y), feature_cols\n",
        "    else:\n",
        "        # Para datos de prueba donde no hay objetivo\n",
        "        return np.array(X), None, feature_cols\n",
        "\n"
      ],
      "metadata": {
        "id": "kFJqBCgh44qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clase para el modelo CNN\n",
        "class DengueForecastCNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Modelo CNN para pronóstico de brotes de dengue.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_channels, sequence_length, num_features):\n",
        "        super(DengueForecastCNN, self).__init__()\n",
        "\n",
        "        self.conv1 = nn.Conv1d(in_channels=input_channels,\n",
        "                               out_channels=64,\n",
        "                               kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv1d(in_channels=64,\n",
        "                               out_channels=128,\n",
        "                               kernel_size=3,\n",
        "                               padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "\n",
        "        # Calcular tamaño de salida después de las capas convolucionales\n",
        "        conv_output_size = sequence_length // 4 * 128\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(conv_output_size, 100)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.2)\n",
        "        self.fc2 = nn.Linear(100, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Entrada: [batch_size, sequence_length, num_features]\n",
        "        # Necesitamos transponer para Conv1d: [batch_size, num_features, sequence_length]\n",
        "        x = x.permute(0, 2, 1)\n",
        "\n",
        "        x = self.pool1(self.relu1(self.conv1(x)))\n",
        "        x = self.pool2(self.relu2(self.conv2(x)))\n",
        "\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(self.relu3(self.fc1(x)))\n",
        "        x = self.fc2(x)\n",
        "\n",
        "        return x.squeeze()\n",
        "\n"
      ],
      "metadata": {
        "id": "EvuqJzF0a3ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para entrenar el modelo\n",
        "def entrenar_modelo(X_train, y_train, X_val, y_val, model, optimizer, criterion, epochs, batch_size=32):\n",
        "\n",
        "    # Convertir a tensores\n",
        "    X_train_tensor = torch.FloatTensor(X_train)\n",
        "    y_train_tensor = torch.FloatTensor(y_train)\n",
        "    X_val_tensor = torch.FloatTensor(X_val)\n",
        "    y_val_tensor = torch.FloatTensor(y_val)\n",
        "\n",
        "    # Crear conjuntos de datos\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    # Listas para almacenar pérdidas\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    # Entrenamiento del modelo\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            # Forward pass\n",
        "            outputs = model(X_batch)\n",
        "            loss = criterion(outputs, y_batch)\n",
        "\n",
        "            # Backward y optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Calcular pérdida promedio de entrenamiento\n",
        "        train_loss = train_loss / len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # Validación\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val_tensor)\n",
        "            val_loss = criterion(val_outputs, y_val_tensor).item()\n",
        "            val_losses.append(val_loss)\n",
        "\n",
        "        # Imprimir progreso cada 10 épocas\n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            print(f'Época {epoch+1}/{epochs}, Pérdida de entrenamiento: {train_loss:.4f}, Pérdida de validación: {val_loss:.4f}')\n",
        "\n",
        "    return model, train_losses, val_losses\n",
        "\n"
      ],
      "metadata": {
        "id": "kU6NxQFHa58E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para evaluar el modelo\n",
        "def evaluar_modelo(model, X_test, y_test):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Convertir a tensor\n",
        "    X_test_tensor = torch.FloatTensor(X_test)\n",
        "\n",
        "    # Predicciones\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X_test_tensor).numpy()\n",
        "\n",
        "    # Calcular métricas\n",
        "    mae = mean_absolute_error(y_test, predictions)\n",
        "    mse = mean_squared_error(y_test, predictions)\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "    # Calcular accuracy (consideramos que predice correctamente si la diferencia es menor a 0.5)\n",
        "    accuracy = np.mean(np.abs(predictions - y_test) < 0.5)\n",
        "\n",
        "    metrics = {\n",
        "        'MAE': mae,\n",
        "        'MSE': mse,\n",
        "        'RMSE': rmse,\n",
        "        'Accuracy': accuracy\n",
        "    }\n",
        "\n",
        "    return metrics, predictions\n",
        "\n"
      ],
      "metadata": {
        "id": "bfOsXBkga765"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para buscar los mejores hiperparámetros\n",
        "def hyperparameter_tuning(X_train, y_train, X_val, y_val, input_channels, sequence_length, num_features):\n",
        "\n",
        "    epochs_list = [100, 300, 500]\n",
        "    learning_rates = [0.01, 0.001]\n",
        "    optimizers = ['Adam', 'SGD', 'RMSprop']\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    best_model = None\n",
        "    best_params = {}\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for epochs in epochs_list:\n",
        "        for lr in learning_rates:\n",
        "            for opt_name in optimizers:\n",
        "                print(f\"\\nProbando con: Épocas={epochs}, LR={lr}, Optimizador={opt_name}\")\n",
        "\n",
        "                # Inicializar modelo\n",
        "                model = DengueForecastCNN(input_channels, sequence_length, num_features)\n",
        "\n",
        "                # Seleccionar optimizador\n",
        "                if opt_name == 'Adam':\n",
        "                    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "                elif opt_name == 'SGD':\n",
        "                    optimizer = optim.SGD(model.parameters(), lr=lr)\n",
        "                else:  # RMSprop\n",
        "                    optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "\n",
        "                # Función de pérdida\n",
        "                criterion = nn.MSELoss()\n",
        "\n",
        "                # Entrenar modelo\n",
        "                model, train_losses, val_losses = entrenar_modelo(\n",
        "                    X_train, y_train, X_val, y_val,\n",
        "                    model, optimizer, criterion, epochs, batch_size=32\n",
        "                )\n",
        "\n",
        "                # Evaluar en validación\n",
        "                final_val_loss = val_losses[-1]\n",
        "\n",
        "                # Guardar resultados\n",
        "                results.append({\n",
        "                    'epochs': epochs,\n",
        "                    'learning_rate': lr,\n",
        "                    'optimizer': opt_name,\n",
        "                    'val_loss': final_val_loss\n",
        "                })\n",
        "\n",
        "                # Actualizar mejor modelo si es necesario\n",
        "                if final_val_loss < best_val_loss:\n",
        "                    best_val_loss = final_val_loss\n",
        "                    best_model = model\n",
        "                    best_params = {\n",
        "                        'epochs': epochs,\n",
        "                        'learning_rate': lr,\n",
        "                        'optimizer': opt_name\n",
        "                    }\n",
        "\n",
        "    # Mostrar resultados de la búsqueda\n",
        "    results_df = pd.DataFrame(results)\n",
        "    print(\"\\nResultados de la búsqueda de hiperparámetros:\")\n",
        "    print(results_df.sort_values('val_loss'))\n",
        "\n",
        "    print(f\"\\nMejores hiperparámetros encontrados:\")\n",
        "    print(f\"Épocas: {best_params['epochs']}\")\n",
        "    print(f\"Learning Rate: {best_params['learning_rate']}\")\n",
        "    print(f\"Optimizador: {best_params['optimizer']}\")\n",
        "    print(f\"Pérdida de validación: {best_val_loss:.4f}\")\n",
        "\n",
        "    return best_model, best_params\n",
        "\n"
      ],
      "metadata": {
        "id": "lXGU4img5se4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir listas de hyperparámetros\n",
        "epochs_list = [100, 300, 500]\n",
        "learning_rates = [0.01, 0.001]\n",
        "optimizers = [optim.Adam, optim.AdamW, optim.SGD, optim.RMSprop]\n",
        "\n",
        "#epochs_list = [100]\n",
        "#learning_rates = [0.01]\n",
        "#optimizers = [optim.Adam]\n"
      ],
      "metadata": {
        "id": "5USrfP0uffSS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Parámetros para las secuencias\n",
        "sequence_length = 12  # 12 semanas de historial\n",
        "forecast_horizon = 3  # Pronóstico a 3 semanas\n",
        "\n",
        "# Preparar secuencias\n",
        "X, y, feature_cols = preparar_sequences(train_df, sequence_length, forecast_horizon)\n",
        "\n",
        "# Dividir en entrenamiento y validación (80/20)\n",
        "split_idx = int(len(X) * 0.8)\n",
        "X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "print(f\"Forma de los datos de entrenamiento: {X_train.shape}\")\n",
        "print(f\"Forma de los datos de validación: {X_val.shape}\")\n",
        "\n",
        "# Parámetros del modelo\n",
        "input_channels = X_train.shape[2]  # Número de características\n",
        "num_features = X_train.shape[1]    # Longitud de la secuencia\n",
        "\n",
        "# Búsqueda de hiperparámetros\n",
        "best_model, best_params = hyperparameter_tuning(\n",
        "  X_train, y_train, X_val, y_val,\n",
        "  input_channels, sequence_length, num_features\n",
        "  )\n",
        "\n",
        "# Evaluar el mejor modelo\n",
        "metrics, _ = evaluate_model(best_model, X_val, y_val)\n",
        "\n",
        "print(\"\\nResumen del modelo:\")\n",
        "print(best_model)\n",
        "\n",
        "print(\"\\nMétricas en el conjunto de validación:\")\n",
        "for metric, value in metrics.items():\n",
        "  print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "  # Preparar datos de prueba\n",
        "  X_test, _, _ = prepare_sequences(test_df, sequence_length, forecast_horizon)\n",
        "\n",
        "  # Hacer predicciones en datos de prueba\n",
        "  best_model.eval()\n",
        "  X_test_tensor = torch.FloatTensor(X_test)\n",
        "  with torch.no_grad():\n",
        "    test_predictions = best_model(X_test_tensor).numpy()\n",
        "\n",
        "  print(\"\\nSe han generado predicciones para los datos de prueba.\")\n",
        "  print(f\"Forma de las predicciones: {test_predictions.shape}\")\n",
        "\n",
        "  # Si hay un archivo de envío, guardar las predicciones\n",
        "  if \"SUBMISSION_DIR\" in config:\n",
        "    submission_df = pd.read_csv(config[\"SUBMISSION_DIR\"])\n",
        "    submission_df['dengue'] = test_predictions\n",
        "    submission_df.to_csv('submission_predicted.csv', index=False)\n",
        "    print(\"Predicciones guardadas en 'submission_predicted.csv'\")"
      ],
      "metadata": {
        "id": "p4ObY72u5wsL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}