{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cam2149/MachineLearningV/blob/main/MLP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equipo**\n",
        "\n",
        "- Nicolás Colmenares\n",
        "\n",
        "- Carlos Martinez"
      ],
      "metadata": {
        "id": "Ckrr43Eb7-jD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Análisis Exploratorio de Datos (EDA) - 0.5 pts\n",
        "\n",
        "  -  Carga y limpieza de los datos.\n",
        "\n",
        "  - Visualización de tendencias y patrones de dengue.\n",
        "\n",
        "  -  Análisis de correlaciones entre variables."
      ],
      "metadata": {
        "id": "eNJUawM57uzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Situación:**\n",
        "Una ciudad enfrenta un aumento significativo de casos de dengue, con una tasa de incidencia que supera el promedio nacional.\n",
        "La anticipación de brotes es crucial para implementar medidas preventivas y reducir la propagación de la enfermedad.\n",
        "\n",
        "**Objetivo:**\n",
        "Desarrollar un modelo predictivo utilizando redes neuronales para pronosticar futuros brotes de dengue en cada barrio de la ciudad.\n",
        "Utilizar una base de datos histórica de casos de dengue desde 2015 hasta 2022 para entrenar el modelo.\n",
        "Anticiparse a los brotes con al menos 3 semanas de anticipación.\n",
        "\n",
        "**Finalidad:**\n",
        "Permitir a las autoridades de salud pública tomar acciones oportunas, como:\n",
        "Preparar a las instituciones prestadoras de salud (IPS).\n",
        "Gestionar recursos (carros fumigadores, limpieza de sumideros).\n",
        "Capacitar a la comunidad."
      ],
      "metadata": {
        "id": "zQ9T88GEx2Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Análisis exploratorio de datos de la serie temporal\n",
        "*   Explicar el comportamiento de la serie temporal en términos cualitativos y cuantitativos para desarrollar intuición para la selección del modelo\n",
        "*    Identificar los modelos candidatos y los posibles parámetros del modelo que se pueden utilizar basándose en los hallazgos del análisis exploratorio de datos"
      ],
      "metadata": {
        "id": "dVtVgKGCyXfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Configuraciones de Colab"
      ],
      "metadata": {
        "id": "l9U_uqnvvYtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mover Kaggle.json a la ubicación correcta después de subirlo"
      ],
      "metadata": {
        "id": "zi1MbjLG81SQ"
      }
    },
    {
      "source": [
        "#Estas líneas son comandos de shell que se ejecutan dentro del Jupyter notebook. Se usan para configurar las credenciales de la API de Kaggle, que son necesarias para descargar conjuntos de datos (datasets) desde Kaggle.\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FkW9-t478uLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/kaggle/output\n",
        "!rm -rf /content/kaggle/input"
      ],
      "metadata": {
        "id": "-YMKmQruoaHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargar dataset de la competencia"
      ],
      "metadata": {
        "id": "VWH9Y2UG9KLD"
      }
    },
    {
      "source": [
        "!kaggle competitions download -c aa-v-2025-i-pronosticos-nn-rnn-cnn"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SnUZl0d-8_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/kaggle/output\n",
        "!mkdir -p /content/kaggle/input"
      ],
      "metadata": {
        "id": "vaLprrm32zGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv aa-v-2025-i-pronosticos-nn-rnn-cnn.zip /content/kaggle/input"
      ],
      "metadata": {
        "id": "s07nq5K1zyuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/kaggle/input/aa-v-2025-i-pronosticos-nn-rnn-cnn.zip -d /content/kaggle/input/"
      ],
      "metadata": {
        "id": "skxqP_oq0VeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/kaggle/input\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "0m2oYgGOxJue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Imports"
      ],
      "metadata": {
        "id": "FiQLr9h-0Cr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install altair"
      ],
      "metadata": {
        "id": "10HK-Am92SqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "import altair as alt\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt   # data visualization\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import scipy\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from scipy.stats import anderson\n",
        "from matplotlib.colors import LinearSegmentedColormap\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, recall_score, roc_auc_score, accuracy_score, roc_curve\n",
        "\n"
      ],
      "metadata": {
        "id": "MO0sH9aq9VlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing library versions\n",
        "print('Pandas:', pd.__version__)\n",
        "print('Statsmodels:', sm.__version__)\n",
        "print('Scipy:', scipy.__version__)\n",
        "print('Matplotlib:', mpl.__version__)\n",
        "print('Seaborn:', sns.__version__)"
      ],
      "metadata": {
        "id": "0AXL-WbwzzVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "np.random.seed(786)"
      ],
      "metadata": {
        "id": "AP-TlyoQ0PMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Configs"
      ],
      "metadata": {
        "id": "ul8b3MgvAEJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"TRAIN_DIR\": '/content/kaggle/input/df_train.parquet',\n",
        "    \"TEST_DIR\": '/content/kaggle/input/df_test.parquet',\n",
        "    \"SUBMISSION_DIR\": '/content/sample_submission.csv'\n",
        "}"
      ],
      "metadata": {
        "id": "tnCzjmsJAGst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exploración"
      ],
      "metadata": {
        "id": "9VCh0_mP9YFN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diccionario"
      ],
      "metadata": {
        "id": "R3PIeauF9c56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.parquet - El conjunto de datos de entrenamiento\n",
        "test.parquet - El conjunto de datos de prueba\n",
        "sample_submission.csv - un ejemplo de un archivo a someter en la competencia"
      ],
      "metadata": {
        "id": "uwGcqMDn-AoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Variable**         | **Descripción**                                                                                      |\n",
        "|-----------------------|------------------------------------------------------------------------------------------------------|\n",
        "| id_bar               | identificador único del barrio                                                                      |\n",
        "| anio                 | Año de ocurrencia                                                                                   |\n",
        "| semana               | Semana de ocurrencia                                                                               |\n",
        "| Estrato              | Estrato socioeconómico del barrio                                                                   |\n",
        "| area_barrio          | Área del barrio en km²                                                                             |\n",
        "| dengue               | Conteo de casos de dengue                                                                          |\n",
        "| concentraciones      | Cantidad de visitas e intervención a lugares de concentración humana (Instituciones)                |\n",
        "| vivienda             | Conteo de las visitas a viviendas a revisión y control de criaderos                                 |\n",
        "| equipesado           | Conteo de las fumigaciones con Maquinaria Pesada                                                   |\n",
        "| sumideros            | Conteo de las intervenciones a los sumideros                                                       |\n",
        "| maquina              | Conteo de las fumigaciones con motomochila                                                         |\n",
        "| lluvia_mean          | Lluvia promedio en la semana i                                                                     |\n",
        "| lluvia_var           | Varianza de la lluvia en la semana i                                                               |\n",
        "| lluvia_max           | Lluvia máxima en la semana i                                                                       |\n",
        "| lluvia_min           | Lluvia mínima en la semana i                                                                       |\n",
        "| temperatura_mean     | Temperatura promedio en la semana i                                                                |\n",
        "| temperatura_var      | Varianza de la temperatura en la semana i                                                          |\n",
        "| temperatura_max      | Temperatura máxima en la semana i                                                                  |\n",
        "| temperatura_min      | Temperatura mínima en la semana i                                                                  |\n"
      ],
      "metadata": {
        "id": "Pw_6kTQ29_UD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lectura del dataset de entrenamiento"
      ],
      "metadata": {
        "id": "eqMzIl1N-EUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Esta celda tiene como objetivo leer los datos de entrenamiento desde un archivo Parquet y mostrar información básica sobre ellos\n",
        "try:\n",
        "    train_df = pd.read_parquet(config[\"TRAIN_DIR\"]).iloc[:,1:]\n",
        "    print(train_df.describe())\n",
        "    print(train_df.info())\n",
        "    print(train_df.shape)\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'series_train.parquet' not found. Please make sure the file exists in the current directory or provide the correct path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred: {e}\")\n"
      ],
      "metadata": {
        "id": "eJ0n4hef5Dd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Función para preparar los datos\n",
        "def preparar_datos(training_df, target_variable, test_size=0.2, batch_size=32):\n",
        "    X = training_df.drop(columns=[target_variable]).values\n",
        "    y = training_df[target_variable].values\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_test = scaler.transform(X_test)\n",
        "\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "    test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "McDN1JqhavSW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir el modelo MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 32)\n",
        "        self.fc3 = nn.Linear(32, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "EvuqJzF0a3ZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para entrenar el modelo\n",
        "def entrenar_modelo(model, train_loader, criterion, optimizer, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')"
      ],
      "metadata": {
        "id": "kU6NxQFHa58E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para evaluar el modelo\n",
        "def evaluar_modelo(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    test_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "    return test_loss"
      ],
      "metadata": {
        "id": "bfOsXBkga765"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para generar el resumen del modelo\n",
        "def generar_resumen(model, train_loader, test_loader, criterion):\n",
        "    model.eval()\n",
        "    train_loss = 0.0\n",
        "    test_loss = 0.0\n",
        "    train_outputs = []\n",
        "    train_targets = []\n",
        "    test_outputs = []\n",
        "    test_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in train_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            train_loss += loss.item() * inputs.size(0)\n",
        "            train_outputs.extend(outputs.view(-1).tolist())\n",
        "            train_targets.extend(targets.view(-1).tolist())\n",
        "\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            test_loss += loss.item() * inputs.size(0)\n",
        "            test_outputs.extend(outputs.view(-1).tolist())\n",
        "            test_targets.extend(targets.view(-1).tolist())\n",
        "\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(f'Train Loss: {train_loss:.4f}')\n",
        "    print(f'Test Loss: {test_loss:.4f}')\n",
        "\n",
        "    # Métricas adicionales\n",
        "    train_outputs = torch.tensor(train_outputs)\n",
        "    train_targets = torch.tensor(train_targets)\n",
        "    test_outputs = torch.tensor(test_outputs)\n",
        "    test_targets = torch.tensor(test_targets)\n",
        "\n",
        "    train_mae = mean_absolute_error(train_targets, train_outputs)\n",
        "    test_mae = mean_absolute_error(test_targets, test_outputs)\n",
        "    train_mse = mean_squared_error(train_targets, train_outputs)\n",
        "    test_mse = mean_squared_error(test_targets, test_outputs)\n",
        "    train_rmse = torch.sqrt(torch.tensor(train_mse)).item()\n",
        "    test_rmse = torch.sqrt(torch.tensor(test_mse)).item()\n",
        "    train_accuracy = accuracy_score((train_outputs > 0.5).int(), (train_targets > 0.5).int())\n",
        "    test_accuracy = accuracy_score((test_outputs > 0.5).int(), (test_targets > 0.5).int())\n",
        "    train_recall = recall_score((train_outputs > 0.5).int(), (train_targets > 0.5).int())\n",
        "    test_recall = recall_score((test_outputs > 0.5).int(), (test_targets > 0.5).int())\n",
        "    train_roc_auc = roc_auc_score((train_targets > 0.5).int(), train_outputs)\n",
        "    test_roc_auc = roc_auc_score((test_targets > 0.5).int(), test_outputs)\n",
        "\n",
        "    print(f'Train MAE: {train_mae:.4f}')\n",
        "    print(f'Test MAE: {test_mae:.4f}')\n",
        "    print(f'Train MSE: {train_mse:.4f}')\n",
        "    print(f'Test MSE: {test_mse:.4f}')\n",
        "    print(f'Train RMSE: {train_rmse:.4f}')\n",
        "    print(f'Test RMSE: {test_rmse:.4f}')\n",
        "    print(f'Train Accuracy: {train_accuracy:.4f}')\n",
        "    print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "    print(f'Train Recall: {train_recall:.4f}')\n",
        "    print(f'Test Recall: {test_recall:.4f}')\n",
        "    print(f'Train ROC AUC: {train_roc_auc:.4f}')\n",
        "    print(f'Test ROC AUC: {test_roc_auc:.4f}')\n",
        "\n",
        "    num_parameters = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f'Number of trainable parameters: {num_parameters}')\n",
        "\n",
        "    print(model)"
      ],
      "metadata": {
        "id": "L0L-V_JecMAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutar todo el pipeline\n",
        "def ejecutar_mlp(training_df, target_variable, epochs=100, batch_size=32, learning_rate=0.01):\n",
        "    train_loader, test_loader = preparar_datos(training_df, target_variable, batch_size=batch_size)\n",
        "    input_dim = training_df.drop(columns=[target_variable]).shape[1]\n",
        "    model = MLP(input_dim)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    entrenar_modelo(model, train_loader, criterion, optimizer, epochs)\n",
        "    generar_resumen(model, train_loader, test_loader, criterion)\n",
        "    test_loss = evaluar_modelo(model, test_loader, criterion)\n",
        "    return model, test_loss"
      ],
      "metadata": {
        "id": "qBJDWJoea-WI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Función para ejecutar el modelo con diferentes combinaciones de hyperparámetros\n",
        "def ejecutar_experimentos(training_df, target_variable, epochs_list, learning_rates):\n",
        "    train_loader, test_loader = preparar_datos(training_df, target_variable)\n",
        "    input_dim = training_df.drop(columns=[target_variable]).shape[1]\n",
        "    best_loss = float('inf')\n",
        "    best_params = None\n",
        "    best_model = None\n",
        "\n",
        "    for epochs in epochs_list:\n",
        "        for lr in learning_rates:\n",
        "            print(f'\\nEntrenando modelo con {epochs} epochs y learning rate de {lr}')\n",
        "            model = MLP(input_dim)\n",
        "            criterion = nn.MSELoss()\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "            entrenar_modelo(model, train_loader, criterion, optimizer, epochs)\n",
        "            test_loss = evaluar_modelo(model, test_loader, criterion)\n",
        "            if test_loss < best_loss:\n",
        "                best_loss = test_loss\n",
        "                best_params = (epochs, lr)\n",
        "                best_model = model\n",
        "\n",
        "    print(f'\\nMejor modelo: {best_params[0]} epochs, learning rate de {best_params[1]}')\n",
        "    print(f'Mejor Test Loss: {best_loss:.4f}')\n",
        "    generar_resumen(best_model, train_loader, test_loader, nn.MSELoss())\n",
        "    return best_model, best_loss, best_params"
      ],
      "metadata": {
        "id": "AnZvANnifciI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Definir listas de hyperparámetros\n",
        "epochs_list = [100, 300, 500, 1000]\n",
        "learning_rates = [0.01, 0.001]\n",
        "\n",
        "# Ejemplo de uso\n",
        "best_model, best_loss, best_params = ejecutar_experimentos(train_df, target_variable='dengue', epochs_list=epochs_list, learning_rates=learning_rates)"
      ],
      "metadata": {
        "id": "5USrfP0uffSS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}