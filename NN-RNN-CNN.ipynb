{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cam2149/MachineLearningV/blob/main/NN-RNN-CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equipo**\n",
        "\n",
        "- Nicolás Colmenares\n",
        "\n",
        "- Carlos Martinez"
      ],
      "metadata": {
        "id": "Ckrr43Eb7-jD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Implementación de una Red Convolucional (CNN) adaptada a series temporales.- 1 pts\n",
        "\n",
        "  -  .\n",
        "\n",
        "  - .\n",
        "\n",
        "  - ."
      ],
      "metadata": {
        "id": "eNJUawM57uzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Situación:**\n",
        "Una ciudad enfrenta un aumento significativo de casos de dengue, con una tasa de incidencia que supera el promedio nacional.\n",
        "La anticipación de brotes es crucial para implementar medidas preventivas y reducir la propagación de la enfermedad.\n",
        "\n",
        "**Objetivo:**\n",
        "Desarrollar un modelo predictivo utilizando redes neuronales para pronosticar futuros brotes de dengue en cada barrio de la ciudad.\n",
        "Utilizar una base de datos histórica de casos de dengue desde 2015 hasta 2022 para entrenar el modelo.\n",
        "Anticiparse a los brotes con al menos 3 semanas de anticipación.\n",
        "\n",
        "**Finalidad:**\n",
        "Permitir a las autoridades de salud pública tomar acciones oportunas, como:\n",
        "Preparar a las instituciones prestadoras de salud (IPS).\n",
        "Gestionar recursos (carros fumigadores, limpieza de sumideros).\n",
        "Capacitar a la comunidad."
      ],
      "metadata": {
        "id": "zQ9T88GEx2Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Red Convolucional (CNN) adaptada a series temporales.\n",
        "*   .\n",
        "*   ."
      ],
      "metadata": {
        "id": "dVtVgKGCyXfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Diccionario"
      ],
      "metadata": {
        "id": "R3PIeauF9c56"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "train.parquet - El conjunto de datos de entrenamiento\n",
        "test.parquet - El conjunto de datos de prueba\n",
        "sample_submission.csv - un ejemplo de un archivo a someter en la competencia"
      ],
      "metadata": {
        "id": "uwGcqMDn-AoF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Variable**         | **Descripción**                                                                                      |\n",
        "|-----------------------|------------------------------------------------------------------------------------------------------|\n",
        "| id_bar               | identificador único del barrio                                                                      |\n",
        "| anio                 | Año de ocurrencia                                                                                   |\n",
        "| semana               | Semana de ocurrencia                                                                               |\n",
        "| Estrato              | Estrato socioeconómico del barrio                                                                   |\n",
        "| area_barrio          | Área del barrio en km²                                                                             |\n",
        "| dengue               | Conteo de casos de dengue                                                                          |\n",
        "| concentraciones      | Cantidad de visitas e intervención a lugares de concentración humana (Instituciones)                |\n",
        "| vivienda             | Conteo de las visitas a viviendas a revisión y control de criaderos                                 |\n",
        "| equipesado           | Conteo de las fumigaciones con Maquinaria Pesada                                                   |\n",
        "| sumideros            | Conteo de las intervenciones a los sumideros                                                       |\n",
        "| maquina              | Conteo de las fumigaciones con motomochila                                                         |\n",
        "| lluvia_mean          | Lluvia promedio en la semana i                                                                     |\n",
        "| lluvia_var           | Varianza de la lluvia en la semana i                                                               |\n",
        "| lluvia_max           | Lluvia máxima en la semana i                                                                       |\n",
        "| lluvia_min           | Lluvia mínima en la semana i                                                                       |\n",
        "| temperatura_mean     | Temperatura promedio en la semana i                                                                |\n",
        "| temperatura_var      | Varianza de la temperatura en la semana i                                                          |\n",
        "| temperatura_max      | Temperatura máxima en la semana i                                                                  |\n",
        "| temperatura_min      | Temperatura mínima en la semana i                                                                  |\n"
      ],
      "metadata": {
        "id": "Pw_6kTQ29_UD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Configuraciones de Colab"
      ],
      "metadata": {
        "id": "l9U_uqnvvYtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mover Kaggle.json a la ubicación correcta después de subirlo"
      ],
      "metadata": {
        "id": "zi1MbjLG81SQ"
      }
    },
    {
      "source": [
        "#Estas líneas son comandos de shell que se ejecutan dentro del Jupyter notebook. Se usan para configurar las credenciales de la API de Kaggle, que son necesarias para descargar conjuntos de datos (datasets) desde Kaggle.\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FkW9-t478uLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/kaggle/output\n",
        "!rm -rf /content/kaggle/input"
      ],
      "metadata": {
        "id": "-YMKmQruoaHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargar dataset de la competencia"
      ],
      "metadata": {
        "id": "VWH9Y2UG9KLD"
      }
    },
    {
      "source": [
        "!kaggle competitions download -c aa-v-2025-i-pronosticos-nn-rnn-cnn"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SnUZl0d-8_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/kaggle/output\n",
        "!mkdir -p /content/kaggle/input"
      ],
      "metadata": {
        "id": "vaLprrm32zGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv aa-v-2025-i-pronosticos-nn-rnn-cnn.zip /content/kaggle/input"
      ],
      "metadata": {
        "id": "s07nq5K1zyuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/kaggle/input/aa-v-2025-i-pronosticos-nn-rnn-cnn.zip -d /content/kaggle/input/"
      ],
      "metadata": {
        "id": "skxqP_oq0VeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/kaggle/input\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "0m2oYgGOxJue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Imports"
      ],
      "metadata": {
        "id": "FiQLr9h-0Cr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, accuracy_score\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "MO0sH9aq9VlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Printing library versions\n",
        "print('Pandas:', pd.__version__)\n",
        "print('Numpy:', np.__version__)\n",
        "print('PyTorch:', torch.__version__)"
      ],
      "metadata": {
        "id": "0AXL-WbwzzVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "AP-TlyoQ0PMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#2. Configuración Inicial y Carga de Datos"
      ],
      "metadata": {
        "id": "ul8b3MgvAEJO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\n",
        "    \"TRAIN_DIR\": '/content/kaggle/input/df_train.parquet',\n",
        "    \"TEST_DIR\": '/content/kaggle/input/df_test.parquet',\n",
        "    \"SUBMISSION_DIR\": '/content/sample_submission.csv',\n",
        "    \"batch_size\" : 32\n",
        "}"
      ],
      "metadata": {
        "id": "tnCzjmsJAGst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuración del dispositivo\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "-BCn_ktm0cF-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar datos\n",
        "train_df = pd.read_parquet(config[\"TRAIN_DIR\"])\n",
        "test_df = pd.read_parquet(config[\"TEST_DIR\"])"
      ],
      "metadata": {
        "id": "R4lWtxiS1gNj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.head(2)"
      ],
      "metadata": {
        "id": "w3-GSMEN1yME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#3. Preprocesamiento de Datos"
      ],
      "metadata": {
        "id": "eNy7oatl37rG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.1 Generar Columna fecha\n",
        "Creamos la columna fecha basada en anio y semana, asignando el último día de cada semana como índice."
      ],
      "metadata": {
        "id": "HYmewRQw4EwQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_week_end_date(row):\n",
        "    return pd.to_datetime(f'{row[\"anio\"]}-W{row[\"semana\"]}-6', format='%Y-W%W-%w')\n",
        "\n",
        "train_df['fecha'] = train_df.apply(get_week_end_date, axis=1)\n",
        "test_df['fecha'] = test_df.apply(get_week_end_date, axis=1)\n",
        "\n",
        "# Establecer 'fecha' como índice\n",
        "train_df.set_index('fecha', inplace=True)\n",
        "test_df.set_index('fecha', inplace=True)"
      ],
      "metadata": {
        "id": "DMjHYBgM37EQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.2 Selección de Características\n",
        "Definimos las características de entrada, considerando las correlaciones altas entre variables (e.g., lluvia_mean y lluvia_var: 0.82). Para simplificar, usamos todas las características disponibles y dejamos que el modelo aprenda las relaciones."
      ],
      "metadata": {
        "id": "JEsuX1x14WXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features = ['ESTRATO', 'area_barrio', 'concentraciones', 'vivienda', 'equipesado', 'sumideros', 'maquina',\n",
        "            'lluvia_mean', 'lluvia_var', 'lluvia_max', 'lluvia_min', 'temperatura_mean', 'temperatura_var',\n",
        "            'temperatura_max', 'temperatura_min']\n",
        "target = 'dengue'"
      ],
      "metadata": {
        "id": "_0b0rdvB4cBz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3 Normalización\n",
        "Normalizamos las características y el target usando MinMaxScaler."
      ],
      "metadata": {
        "id": "KtgONUzG4fZb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "scaler = MinMaxScaler()\n",
        "train_df[features] = scaler.fit_transform(train_df[features])\n",
        "test_df[features] = scaler.transform(test_df[features])\n",
        "\n",
        "target_scaler = MinMaxScaler()\n",
        "train_df[target] = target_scaler.fit_transform(train_df[[target]])"
      ],
      "metadata": {
        "id": "g9SU3JzO4jua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.3 Crear Secuencias para Series Temporales\n",
        "Para predecir con 3 semanas de anticipación, usamos una ventana de 5 semanas (window_size=5) y un horizonte de 3 semanas (horizon=3)."
      ],
      "metadata": {
        "id": "uea5HBGF5y9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "window_size = 5\n",
        "horizon = 3\n",
        "\n",
        "train_df = train_df.sort_values(by=['id_bar', 'fecha'])\n",
        "\n",
        "sequences = []\n",
        "targets = []\n",
        "\n",
        "for id_bar, group in train_df.groupby('id_bar'):\n",
        "    group = group.sort_values('fecha')\n",
        "    for i in range(window_size, len(group) - horizon + 1):\n",
        "        seq = group[features].iloc[i - window_size:i].values\n",
        "        target_val = group[target].iloc[i + horizon - 1]\n",
        "        sequences.append(seq)\n",
        "        targets.append(target_val)\n",
        "\n",
        "sequences = np.array(sequences)\n",
        "targets = np.array(targets)"
      ],
      "metadata": {
        "id": "EuEmBfZ-56bR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##3.4 Dataset y DataLoader\n",
        "Creamos un Dataset personalizado y dividimos en entrenamiento y validación."
      ],
      "metadata": {
        "id": "QoVFGjmM5_bJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TimeSeriesDataset(Dataset):\n",
        "    def __init__(self, sequences, targets):\n",
        "        self.sequences = sequences\n",
        "        self.targets = targets\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.sequences[idx], dtype=torch.float32), torch.tensor(self.targets[idx], dtype=torch.float32)\n",
        "\n",
        "num_sequences = len(sequences)\n",
        "train_size = int(0.8 * num_sequences)\n",
        "train_sequences, val_sequences = sequences[:train_size], sequences[train_size:]\n",
        "train_targets, val_targets = targets[:train_size], targets[train_size:]\n",
        "\n",
        "train_dataset = TimeSeriesDataset(train_sequences, train_targets)\n",
        "val_dataset = TimeSeriesDataset(val_sequences, val_targets)\n",
        "\n",
        "batch_size = config[\"batch_size\"]\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "EM4JWY_o6EYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4. Implementación de Modelos"
      ],
      "metadata": {
        "id": "VbrI6Wbo6ljL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##4.1 Modelo MLP\n",
        "Un Perceptrón Multicapa que aplana las secuencias."
      ],
      "metadata": {
        "id": "dhYPwPA66pMK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        return out\n",
        "\n",
        "input_dim = window_size * len(features)\n",
        "hidden_dim = 64\n",
        "output_dim = 1\n",
        "mlp_model = MLPModel(input_dim, hidden_dim, output_dim).to(DEVICE)"
      ],
      "metadata": {
        "id": "KXQl0Ajm6sIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.2 Modelo CNN para Series Temporales\n",
        "Una CNN 1D adaptada a series temporales."
      ],
      "metadata": {
        "id": "YfsmgmEq7H1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNModel(nn.Module):\n",
        "    def __init__(self, num_features, hidden_dim, output_dim):\n",
        "        super(CNNModel, self).__init__()\n",
        "        self.conv1 = nn.Conv1d(num_features, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool1d(2)\n",
        "        self.conv2 = nn.Conv1d(hidden_dim, hidden_dim, kernel_size=3, padding=1)\n",
        "        self.fc = nn.Linear(hidden_dim * (window_size // 2), output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.permute(0, 2, 1)\n",
        "        out = self.conv1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.pool(out)\n",
        "        out = self.conv2(out)\n",
        "        out = self.relu(out)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.fc(out)\n",
        "        return out\n",
        "\n",
        "cnn_model = CNNModel(len(features), hidden_dim, output_dim).to(DEVICE)"
      ],
      "metadata": {
        "id": "s_fFrymn7KC2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.3 Modelo RNN Básico\n",
        "Implementación proporcionada con estados iniciales definidos."
      ],
      "metadata": {
        "id": "RXjpZx3m7M0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RNNModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.rnn = nn.RNN(input_dim, hidden_dim, layer_dim, batch_first=True, nonlinearity='relu')\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(DEVICE)\n",
        "        out, hn = self.rnn(x, h0)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "rnn_model = RNNModel(len(features), hidden_dim, 1, output_dim).to(DEVICE)"
      ],
      "metadata": {
        "id": "wY1xcLSi7PJ4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.4 Modelo LSTM"
      ],
      "metadata": {
        "id": "91i05YyF7RhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True).to(DEVICE)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim).to(DEVICE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(DEVICE)\n",
        "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(DEVICE)\n",
        "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "lstm_model = LSTMModel(len(features), hidden_dim, 1, output_dim).to(DEVICE)"
      ],
      "metadata": {
        "id": "1mipmz3M7V7s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.5 Modelo GRU"
      ],
      "metadata": {
        "id": "EAtxxjkp7awI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GRUModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
        "        super(GRUModel, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.layer_dim = layer_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, layer_dim, batch_first=True).to(DEVICE)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim).to(DEVICE)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(DEVICE)\n",
        "        out, hn = self.gru(x, h0.detach())\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "gru_model = GRUModel(len(features), hidden_dim, 1, output_dim).to(DEVICE)"
      ],
      "metadata": {
        "id": "zIYK3qdt7c6O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#5. Entrenamiento y Evaluación"
      ],
      "metadata": {
        "id": "uTEzHGf97gyT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.1 Función de Entrenamiento"
      ],
      "metadata": {
        "id": "__VP-McM7jYO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_loader, val_loader, epochs, optimizer, criterion):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "\n",
        "        # Create a progress bar for the training loop\n",
        "        train_pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{epochs} [Train]', leave=False)\n",
        "\n",
        "        for x, y in train_pbar:\n",
        "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(x)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Update progress bar\n",
        "            train_pbar.set_postfix({'loss': f'{loss.item():.4f}'})\n",
        "\n",
        "        train_loss /= len(train_loader)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        val_loss = 0\n",
        "\n",
        "        # Create a progress bar for the validation loop\n",
        "        val_pbar = tqdm(val_loader, desc=f'Epoch {epoch+1}/{epochs} [Val]', leave=False)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, y in val_pbar:\n",
        "                x, y = x.to(DEVICE), y.to(DEVICE)\n",
        "                output = model(x)\n",
        "                batch_loss = criterion(output, y).item()\n",
        "                val_loss += batch_loss\n",
        "\n",
        "                # Update progress bar\n",
        "                val_pbar.set_postfix({'loss': f'{batch_loss:.4f}'})\n",
        "\n",
        "        val_loss /= len(val_loader)\n",
        "        val_losses.append(val_loss)\n",
        "\n",
        "        if (epoch + 1) % 10 == 0:  # Imprimir solo cada 10 épocas\n",
        "            print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
        "\n",
        "    return train_losses, val_losses"
      ],
      "metadata": {
        "id": "5u1w28uH7lcI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.2 Función de Evaluación"
      ],
      "metadata": {
        "id": "PxuM2Xd47vji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, val_loader):\n",
        "    model.eval()\n",
        "    preds, actuals = [], []\n",
        "    with torch.no_grad():\n",
        "        for x, y in val_loader:\n",
        "            x = x.to(DEVICE)\n",
        "            output = model(x)\n",
        "            preds.extend(output.cpu().numpy())\n",
        "            actuals.extend(y.numpy())\n",
        "    preds = target_scaler.inverse_transform(np.array(preds).reshape(-1, 1))\n",
        "    actuals = target_scaler.inverse_transform(np.array(actuals).reshape(-1, 1))\n",
        "    mae = mean_absolute_error(actuals, preds)\n",
        "    mse = mean_squared_error(actuals, preds)\n",
        "    rmse = np.sqrt(mse)\n",
        "    print(f'MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}')\n",
        "    return mae, mse, rmse, preds, actuals"
      ],
      "metadata": {
        "id": "NacHAVl_71M6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.3 Entrenar Modelos\n",
        "Entrenamos cada modelo con hiperparámetros fijos para comparación inicial."
      ],
      "metadata": {
        "id": "TcSszRDU749v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "models = {'MLP': mlp_model, 'CNN': cnn_model, 'RNN': rnn_model, 'LSTM': lstm_model, 'GRU': gru_model}\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f'\\nEntrenando {name}...')\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    train_losses, val_losses = train_model(model, train_loader, val_loader, epochs=100, optimizer=optimizer, criterion=criterion)\n",
        "    mae, mse, rmse, preds, actuals = evaluate_model(model, val_loader)\n",
        "    results[name] = {'train_losses': train_losses, 'val_losses': val_losses, 'mae': mae, 'mse': mse, 'rmse': rmse}"
      ],
      "metadata": {
        "id": "4WPypA4979SN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##5.4 Gráficos\n",
        "Generamos gráficos de pérdidas y predicciones vs reales."
      ],
      "metadata": {
        "id": "UpDtMX208FxH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_losses(train_losses, val_losses, title):\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Val Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "def plot_predictions(actuals, preds, title):\n",
        "    plt.plot(actuals[:100], label='Actual')\n",
        "    plt.plot(preds[:100], label='Predicted')\n",
        "    plt.xlabel('Sample')\n",
        "    plt.ylabel('Dengue Cases')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "for name in models.keys():\n",
        "    plot_losses(results[name]['train_losses'], results[name]['val_losses'], f'{name} Losses')\n",
        "    _, _, _, preds, actuals = evaluate_model(models[name], val_loader)\n",
        "    plot_predictions(actuals, preds, f'{name} Predictions vs Actual')"
      ],
      "metadata": {
        "id": "kDnTvwsS8FMl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#6. Selección del Mejor Modelo\n",
        "Realizamos una búsqueda simple sobre hiperparámetros para cada modelo y seleccionamos el mejor basado en RMSE."
      ],
      "metadata": {
        "id": "G9GXaihI8Nh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs_list = [100, 300, 500]\n",
        "learning_rates = [0.01, 0.001]\n",
        "optimizers = [optim.Adam, optim.AdamW]\n",
        "batch_sizes = [16, 32, 48]\n",
        "\n",
        "best_model_name = None\n",
        "best_rmse = float('inf')\n",
        "best_config = {}\n",
        "\n",
        "for name, model_class in {'MLP': MLPModel, 'CNN': CNNModel, 'RNN': RNNModel, 'LSTM': LSTMModel, 'GRU': GRUModel}.items():\n",
        "  print(f'\\nEntrenando {name}...')\n",
        "  for epochs in epochs_list:\n",
        "      for lr in learning_rates:\n",
        "          for opt_class in optimizers:\n",
        "              for bs in batch_sizes:\n",
        "                  if name == 'MLP':\n",
        "                      model = model_class(input_dim, hidden_dim, output_dim).to(DEVICE)\n",
        "                  else:\n",
        "                      model = model_class(len(features), hidden_dim, 1, output_dim).to(DEVICE)\n",
        "                  train_loader = DataLoader(train_dataset, batch_size=bs, shuffle=True)\n",
        "                  val_loader = DataLoader(val_dataset, batch_size=bs, shuffle=False)\n",
        "                  optimizer = opt_class(model.parameters(), lr=lr)\n",
        "                  train_losses, val_losses = train_model(model, train_loader, val_loader, epochs, optimizer, criterion)\n",
        "                  _, _, rmse, _, _ = evaluate_model(model, val_loader)\n",
        "                  if rmse < best_rmse:\n",
        "                      best_rmse = rmse\n",
        "                      best_model_name = name\n",
        "                      best_config = {'epochs': epochs, 'lr': lr, 'optimizer': opt_class.__name__, 'batch_size': bs}\n",
        "\n",
        "print(f'Mejor modelo: {best_model_name}, RMSE: {best_rmse:.4f}, Config: {best_config}')"
      ],
      "metadata": {
        "id": "0eRBn-qW8MZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#7. Predicción en el Test Set con MLP\n",
        "Evaluamos el modelo MLP en el conjunto de test."
      ],
      "metadata": {
        "id": "LVdjsQW78pIc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.1 Crear Secuencias para Test\n",
        "Combinamos train y test para obtener las semanas previas necesarias."
      ],
      "metadata": {
        "id": "etAZbHts8uo3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_df = pd.concat([train_df.drop(columns=[target]), test_df], sort=False)\n",
        "combined_df = combined_df.sort_values(by=['id_bar', 'fecha'])\n",
        "\n",
        "test_sequences = []\n",
        "test_ids = []\n",
        "\n",
        "for idx, row in test_df.iterrows():\n",
        "    id_bar = row['id_bar']\n",
        "    fecha = row.name\n",
        "    prev_dates = combined_df[(combined_df['id_bar'] == id_bar) & (combined_df.index < fecha)].tail(window_size)\n",
        "    if len(prev_dates) == window_size:\n",
        "        seq = prev_dates[features].values\n",
        "        test_sequences.append(seq)\n",
        "        test_ids.append(row['id'])\n",
        "\n",
        "test_sequences = np.array(test_sequences)\n",
        "test_tensor = torch.tensor(test_sequences, dtype=torch.float32).to(DEVICE)"
      ],
      "metadata": {
        "id": "ynhDKcV98ys6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##7.2 Entrenar MLP Final y Predecir"
      ],
      "metadata": {
        "id": "sgoAdOEI83Vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mlp_model = MLPModel(input_dim, hidden_dim, output_dim).to(DEVICE)\n",
        "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "train_losses, val_losses = train_model(mlp_model, train_loader, val_loader, epochs=100, optimizer=optimizer, criterion=criterion)\n",
        "\n",
        "mlp_model.eval()\n",
        "with torch.no_grad():\n",
        "    preds = mlp_model(test_tensor).cpu().numpy()\n",
        "preds = target_scaler.inverse_transform(preds)\n",
        "\n",
        "submission = pd.DataFrame({'id': test_ids, 'dengue': preds.flatten()})\n",
        "submission.to_csv(config[\"SUBMISSION_DIR\"], index=False)\n",
        "print(\"Archivo submission.csv generado exitosamente.\")"
      ],
      "metadata": {
        "id": "tDY0iw4X856w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#8. Resumen del Modelo"
      ],
      "metadata": {
        "id": "_stJiprO8-_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for name in results.keys():\n",
        "    print(f\"\\nResumen del modelo {name}:\")\n",
        "    print(f\"MAE: {results[name]['mae']:.4f}\")\n",
        "    print(f\"MSE: {results[name]['mse']:.4f}\")\n",
        "    print(f\"RMSE: {results[name]['rmse']:.4f}\")\n",
        "    print(\"Nota: Accuracy no aplica directamente en regresión; se usaron métricas MAE, MSE y RMSE.\")"
      ],
      "metadata": {
        "id": "shlo0l8K9Boz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}