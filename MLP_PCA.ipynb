{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cam2149/MachineLearningV/blob/main/MLP_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Equipo**\n",
        "\n",
        "- Nicolás Colmenares\n",
        "\n",
        "- Carlos Martinez"
      ],
      "metadata": {
        "id": "Ckrr43Eb7-jD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Modelo con Redes Neuronales Tradicionales (MLP)  - 0.5 pts\n",
        "\n",
        "  -  .\n",
        "\n",
        "  - .\n",
        "\n",
        "  - ."
      ],
      "metadata": {
        "id": "eNJUawM57uzy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Situación:**\n",
        "Una ciudad enfrenta un aumento significativo de casos de dengue, con una tasa de incidencia que supera el promedio nacional.\n",
        "La anticipación de brotes es crucial para implementar medidas preventivas y reducir la propagación de la enfermedad.\n",
        "\n",
        "**Objetivo:**\n",
        "Desarrollar un modelo predictivo utilizando redes neuronales para pronosticar futuros brotes de dengue en cada barrio de la ciudad.\n",
        "Utilizar una base de datos histórica de casos de dengue desde 2015 hasta 2022 para entrenar el modelo.\n",
        "Anticiparse a los brotes con al menos 3 semanas de anticipación.\n",
        "\n",
        "**Finalidad:**\n",
        "Permitir a las autoridades de salud pública tomar acciones oportunas, como:\n",
        "Preparar a las instituciones prestadoras de salud (IPS).\n",
        "Gestionar recursos (carros fumigadores, limpieza de sumideros).\n",
        "Capacitar a la comunidad."
      ],
      "metadata": {
        "id": "zQ9T88GEx2Qt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Modelo con Redes Neuronales Tradicionales (MLP)\n",
        "*   .\n",
        "*   ."
      ],
      "metadata": {
        "id": "dVtVgKGCyXfH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0. Configuraciones de Colab"
      ],
      "metadata": {
        "id": "l9U_uqnvvYtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mover Kaggle.json a la ubicación correcta después de subirlo"
      ],
      "metadata": {
        "id": "zi1MbjLG81SQ"
      }
    },
    {
      "source": [
        "#Estas líneas son comandos de shell que se ejecutan dentro del Jupyter notebook. Se usan para configurar las credenciales de la API de Kaggle, que son necesarias para descargar conjuntos de datos (datasets) desde Kaggle.\n",
        "\n",
        "!mkdir -p ~/.kaggle\n",
        "!mv kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "FkW9-t478uLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/kaggle/output\n",
        "!rm -rf /content/kaggle/input"
      ],
      "metadata": {
        "id": "-YMKmQruoaHz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargar dataset de la competencia"
      ],
      "metadata": {
        "id": "VWH9Y2UG9KLD"
      }
    },
    {
      "source": [
        "!kaggle competitions download -c aa-v-2025-i-pronosticos-nn-rnn-cnn"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SnUZl0d-8_Cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/kaggle/output\n",
        "!mkdir -p /content/kaggle/input"
      ],
      "metadata": {
        "id": "vaLprrm32zGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mv aa-v-2025-i-pronosticos-nn-rnn-cnn.zip /content/kaggle/input"
      ],
      "metadata": {
        "id": "s07nq5K1zyuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/kaggle/input/aa-v-2025-i-pronosticos-nn-rnn-cnn.zip -d /content/kaggle/input/"
      ],
      "metadata": {
        "id": "skxqP_oq0VeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#/kaggle/input\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/content/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n"
      ],
      "metadata": {
        "id": "0m2oYgGOxJue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Imports"
      ],
      "metadata": {
        "id": "FiQLr9h-0Cr7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy -y"
      ],
      "metadata": {
        "id": "Ag3ntDYI2pBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "print(np.__version__)"
      ],
      "metadata": {
        "id": "DmFL141I2rwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --force-reinstall numpy==1.26.4"
      ],
      "metadata": {
        "id": "aisgCR642vXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install darts"
      ],
      "metadata": {
        "id": "Hil84i1c2ysP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import optuna\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "\n",
        "config = {\n",
        "    \"TRAIN_DIR\": '/content/kaggle/input/df_train.parquet',\n",
        "    \"TEST_DIR\": '/content/kaggle/input/df_test.parquet',\n",
        "    \"SUBMISSION_DIR\": '/content/sample_submission.csv',\n",
        "    \"BATCH_SIZE\": 32,\n",
        "    \"TARGET_COLUMN\": 'dengue',\n",
        "    \"GROUP_COLUMN\": 'id_bar',\n",
        "    \"WINDOW_SIZE\": 5,\n",
        "    \"HORIZON\": 3,\n",
        "}\n",
        "\n",
        "train_full = pd.read_parquet(config[\"TRAIN_DIR\"])\n",
        "test_df = pd.read_parquet(config[\"TEST_DIR\"])"
      ],
      "metadata": {
        "id": "TBMUn6NwsLLM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generar columna 'fecha' (último día de la semana, domingo)\n",
        "def last_day_of_week(year, week):\n",
        "    first_day = datetime.strptime(f'{year} {week} 1', \"%Y %W %w\")\n",
        "    days_ahead = 6 - first_day.weekday()\n",
        "    last_day = first_day + timedelta(days=days_ahead)\n",
        "    return last_day\n",
        "\n",
        "train_df['fecha'] = train_df.apply(lambda row: last_day_of_week(row['anio'], row['semana']), axis=1)\n",
        "test_df['fecha'] = test_df.apply(lambda row: last_day_of_week(row['anio'], row['semana']), axis=1)"
      ],
      "metadata": {
        "id": "ZZDzAdNAsVKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.index = pd.to_datetime(train_df['fecha'])\n",
        "test_df.index = pd.to_datetime(test_df['fecha'])"
      ],
      "metadata": {
        "id": "TXoZWfvEL_yH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.index"
      ],
      "metadata": {
        "id": "vARZoKVZMZ0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df.resample('W').sum(numeric_only=True)\n"
      ],
      "metadata": {
        "id": "161DPNugMh-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_df = train_full[train_full['anio'] <= 2020].copy()\n",
        "val_df = train_full[train_full['anio'] >= 2021].copy()\n",
        "# test_df ya está cargado como el conjunto de prueba (2022)"
      ],
      "metadata": {
        "id": "-Z-sHDxctHo3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to convert a DataFrame column to TimeSeries\n",
        "def df_col_to_timeseries(df, col_name, freq='W-SUN'):\n",
        "    \"\"\"Converts a DataFrame column to a TimeSeries object.\"\"\"\n",
        "    return TimeSeries.from_series(df[col_name], freq=freq)\n"
      ],
      "metadata": {
        "id": "gqUYfO3U3uzu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lluvia_cols = ['lluvia_mean', 'lluvia_var', 'lluvia_max', 'lluvia_min']\n",
        "temperatura_cols = ['temperatura_mean', 'temperatura_var', 'temperatura_max', 'temperatura_min']\n",
        "other_cols = ['ESTRATO', 'area_barrio', 'concentraciones', 'vivienda', 'equipesado', 'sumideros', 'maquina']\n",
        "target_col = ['dengue']\n",
        "\n",
        "# Escalar variables de lluvia\n",
        "scaler_lluvia = StandardScaler()\n",
        "train_df[lluvia_cols] = scaler_lluvia.fit_transform(train_df[lluvia_cols])\n",
        "val_df[lluvia_cols] = scaler_lluvia.transform(val_df[lluvia_cols])\n",
        "test_df[lluvia_cols] = scaler_lluvia.transform(test_df[lluvia_cols])\n",
        "\n",
        "pca_lluvia = PCA(n_components=0.95)\n",
        "train_lluvia_pca = pca_lluvia.fit_transform(train_df[lluvia_cols])\n",
        "val_lluvia_pca = pca_lluvia.transform(val_df[lluvia_cols])\n",
        "test_lluvia_pca = pca_lluvia.transform(test_df[lluvia_cols])\n",
        "n_components_lluvia = pca_lluvia.n_components_\n",
        "print(f\"Componentes PCA Lluvia: {n_components_lluvia}\")\n",
        "\n",
        "# Escalar variables de temperatura\n",
        "scaler_temperatura = StandardScaler()\n",
        "train_df[temperatura_cols] = scaler_temperatura.fit_transform(train_df[temperatura_cols])\n",
        "val_df[temperatura_cols] = scaler_temperatura.transform(val_df[temperatura_cols])\n",
        "test_df[temperatura_cols] = scaler_temperatura.transform(test_df[temperatura_cols])\n",
        "\n",
        "pca_temperatura = PCA(n_components=0.95)\n",
        "train_temperatura_pca = pca_temperatura.fit_transform(train_df[temperatura_cols])\n",
        "val_temperatura_pca = pca_temperatura.transform(val_df[temperatura_cols])\n",
        "test_temperatura_pca = pca_temperatura.transform(test_df[temperatura_cols])\n",
        "n_components_temperatura = pca_temperatura.n_components_\n",
        "print(f\"Componentes PCA Temperatura: {n_components_temperatura}\")\n",
        "\n",
        "# Escalar otras variables\n",
        "scaler_other = StandardScaler()\n",
        "train_df[other_cols] = scaler_other.fit_transform(train_df[other_cols])\n",
        "val_df[other_cols] = scaler_other.transform(val_df[other_cols])\n",
        "test_df[other_cols] = scaler_other.transform(test_df[other_cols])\n",
        "\n",
        "# Escalar target\n",
        "scaler_target = StandardScaler()\n",
        "train_df[target_col] = scaler_target.fit_transform(train_df[target_col])\n",
        "val_df[target_col] = scaler_target.transform(val_df[target_col])"
      ],
      "metadata": {
        "id": "gTVN0VIrtJor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para val_df\n",
        "X_val, y_val = [], []\n",
        "for barrio in barrios:\n",
        "    barrio_data = val_df[val_df['id_bar'] == barrio]\n",
        "    # Obtener las posiciones de barrio_data en val_df\n",
        "    barrio_pos = val_df.index.get_indexer(barrio_data.index)\n",
        "    # Usar posiciones en lugar de índices para acceder a los arreglos PCA\n",
        "    barrio_features = np.hstack((val_lluvia_pca[barrio_pos],\n",
        "                                 val_temperatura_pca[barrio_pos],\n",
        "                                 barrio_data[other_cols].values))\n",
        "    barrio_target = barrio_data['dengue'].values\n",
        "    if len(barrio_data) >= config[\"WINDOW_SIZE\"] + config[\"HORIZON\"]:\n",
        "        X_barrio, y_barrio = create_sequences(barrio_features, barrio_target,\n",
        "                                              config[\"WINDOW_SIZE\"], config[\"HORIZON\"])\n",
        "        X_val.append(X_barrio)\n",
        "        y_val.append(y_barrio)\n",
        "X_val = np.vstack(X_val)\n",
        "y_val = np.vstack(y_val)"
      ],
      "metadata": {
        "id": "n81Ih1hHuTRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPModel(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout_rate=0.2):\n",
        "        super(MLPModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(hidden_dim, output_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.view(batch_size, -1)  # Aplanar: (batch, window_size * features)\n",
        "        return self.model(x)\n",
        ""
      ],
      "metadata": {
        "id": "ZJqgqul1ujXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def objective(trial):\n",
        "    epochs = trial.suggest_categorical('epochs', [100])\n",
        "    lr = trial.suggest_categorical('lr', [0.01, 0.001])\n",
        "    optimizer_class = trial.suggest_categorical('optimizer', [optim.Adam, optim.AdamW, optim.SGD, optim.RMSprop])\n",
        "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 48])\n",
        "    hidden_dim = trial.suggest_categorical('hidden_dim', [32, 64, 128])\n",
        "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
        "\n",
        "    train_loader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
        "                                            torch.tensor(y_train, dtype=torch.float32)),\n",
        "                              batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
        "                                          torch.tensor(y_val, dtype=torch.float32)),\n",
        "                            batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    input_dim = X_train.shape[1] * X_train.shape[2]\n",
        "    output_dim = config[\"HORIZON\"]\n",
        "    model = MLPModel(input_dim, hidden_dim, output_dim, dropout_rate)\n",
        "    optimizer = optimizer_class(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    model.eval()\n",
        "    val_losses = []\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            y_pred = model(X_batch)\n",
        "            loss = criterion(y_pred, y_batch)\n",
        "            val_losses.append(loss.item())\n",
        "    val_rmse = np.sqrt(np.mean(val_losses))\n",
        "    return val_rmse\n",
        "\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective, n_trials=50)\n",
        "best_params = study.best_params\n",
        "print(\"Mejores parámetros:\", best_params)"
      ],
      "metadata": {
        "id": "C3i4X6QwumZO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_dim = X_train.shape[1] * X_train.shape[2]\n",
        "output_dim = config[\"HORIZON\"]\n",
        "model = MLPModel(input_dim, best_params['hidden_dim'], output_dim, best_params['dropout_rate'])\n",
        "optimizer = eval(f\"optim.{best_params['optimizer'].__name__}\")(model.parameters(), lr=best_params['lr'])\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "train_loader = DataLoader(TensorDataset(torch.tensor(X_train, dtype=torch.float32),\n",
        "                                        torch.tensor(y_train, dtype=torch.float32)),\n",
        "                          batch_size=best_params['batch_size'], shuffle=True)\n",
        "val_loader = DataLoader(TensorDataset(torch.tensor(X_val, dtype=torch.float32),\n",
        "                                      torch.tensor(y_val, dtype=torch.float32)),\n",
        "                        batch_size=best_params['batch_size'], shuffle=False)\n",
        "\n",
        "train_losses, val_losses = [], []\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "counter = 0\n",
        "\n",
        "for epoch in range(best_params['epochs']):\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        y_pred = model(X_batch)\n",
        "        loss = criterion(y_pred, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "    train_losses.append(epoch_loss / len(train_loader))\n",
        "\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            y_pred = model(X_batch)\n",
        "            val_loss += criterion(y_pred, y_batch).item()\n",
        "    val_loss /= len(val_loader)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        counter = 0\n",
        "        torch.save(model.state_dict(), 'mejor_modelo.pt')\n",
        "    else:\n",
        "        counter += 1\n",
        "        if counter >= patience:\n",
        "            print(f\"Early stopping en epoch {epoch}\")\n",
        "            break"
      ],
      "metadata": {
        "id": "dJuA5EWqwKAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load('mejor_modelo.pt'))\n",
        "model.eval()\n",
        "y_pred_val = []\n",
        "y_true_val = []\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in val_loader:\n",
        "        y_pred = model(X_batch)\n",
        "        y_pred_val.extend(y_pred[:, 0].cpu().numpy())  # Primer horizonte\n",
        "        y_true_val.extend(y_batch[:, 0].cpu().numpy())\n",
        "\n",
        "y_pred_val = scaler_target.inverse_transform(np.array(y_pred_val).reshape(-1, 1))\n",
        "y_true_val = scaler_target.inverse_transform(np.array(y_true_val).reshape(-1, 1))\n",
        "\n",
        "mae = mean_absolute_error(y_true_val, y_pred_val)\n",
        "mse = mean_squared_error(y_true_val, y_pred_val)\n",
        "rmse = np.sqrt(mse)\n",
        "print(f\"MAE: {mae:.4f}, MSE: {mse:.4f}, RMSE: {rmse:.4f}\")"
      ],
      "metadata": {
        "id": "7YJ_mBpqwPSk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Train Loss')\n",
        "plt.plot(val_losses, label='Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Losses')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YSk0lqSYwS-r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(y_true_val[:100], label='Real')\n",
        "plt.plot(y_pred_val[:100], label='Predicho')\n",
        "plt.xlabel('Muestra')\n",
        "plt.ylabel('Casos de Dengue')\n",
        "plt.title('Predicciones vs Valores Reales (Primeras 100 muestras)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0y5m-fDewVxw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test = []\n",
        "test_indices = []\n",
        "for barrio in test_df['id_bar'].unique():\n",
        "    barrio_full = pd.concat([train_full[train_full['id_bar'] == barrio],\n",
        "                             test_df[test_df['id_bar'] == barrio]])\n",
        "    # Reset index to avoid using original indices from train_full and test_df\n",
        "    barrio_full.reset_index(drop=True, inplace=True)\n",
        "\n",
        "    idx_full = np.arange(len(barrio_full))\n",
        "    # Use iloc to access data based on the position in barrio_full\n",
        "    barrio_features_full = np.hstack((\n",
        "        pca_lluvia.transform(barrio_full[lluvia_cols].iloc[:].values),\n",
        "        pca_temperatura.transform(barrio_full[temperatura_cols].iloc[:].values),\n",
        "        barrio_full[other_cols].iloc[:].values\n",
        "    ))\n",
        "    barrio_target_full = barrio_full['dengue'].fillna(0).values\n",
        "    test_start_idx = len(train_full[train_full['id_bar'] == barrio])\n",
        "    for i in range(test_start_idx, len(barrio_full)):\n",
        "        if i - config[\"WINDOW_SIZE\"] >= 0:\n",
        "            X_seq = np.hstack((barrio_features_full[i - config[\"WINDOW_SIZE\"]:i],\n",
        "                               barrio_target_full[i - config[\"WINDOW_SIZE\"]:i].reshape(-1, 1)))\n",
        "            X_test.append(X_seq)\n",
        "            test_indices.append(barrio_full.index[i])\n",
        "X_test = np.array(X_test)\n",
        "\n",
        "test_loader = DataLoader(TensorDataset(torch.tensor(X_test, dtype=torch.float32)),\n",
        "                         batch_size=best_params['batch_size'], shuffle=False)\n",
        "\n",
        "model.eval()\n",
        "predictions = []\n",
        "with torch.no_grad():\n",
        "    for X_batch in test_loader:\n",
        "        y_pred = model(X_batch[0])\n",
        "        predictions.extend(y_pred[:, 0].cpu().numpy())  # Primer horizonte\n",
        "\n",
        "predictions = scaler_target.inverse_transform(np.array(predictions).reshape(-1, 1)).flatten()"
      ],
      "metadata": {
        "id": "4PhsSi8mwYaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_submission(test_df, predictions, filename='submission.csv'):\n",
        "    test_df_reset = test_df.reset_index()\n",
        "    ids = test_df_reset['id_bar'].astype(str) + '_' + test_df_reset['anio'].astype(str) + '_' + test_df_reset['semana'].astype(str)\n",
        "    df_submission = pd.DataFrame({'id': ids, 'dengue': predictions})\n",
        "    df_submission.to_csv(filename, index=False)\n",
        "    print(f'Submission guardado en {filename}, con {len(df_submission)} predicciones.')\n",
        "\n",
        "create_submission(test_df, predictions)"
      ],
      "metadata": {
        "id": "kZgCCOA9wu1S"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}